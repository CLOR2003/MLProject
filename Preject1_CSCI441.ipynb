{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4df3253",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CLOR2003/MLProject/blob/Lor/Copy_of_Preject1_CSCI441.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07",
      "metadata": {
        "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07"
      },
      "source": [
        "# Project 1\n",
        "## CSCI 441\n",
        "## Group 2\n",
        "## Members: Chakong Lor, Zachary Sunder, Luis Aguilar\n",
        "## Date: 10/4/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef472cdd-a7da-49c2-88c7-b3d8cfa56fdc",
      "metadata": {
        "id": "ef472cdd-a7da-49c2-88c7-b3d8cfa56fdc"
      },
      "source": [
        "### Frame the Problem and Look at the Big Picture\n",
        "\n",
        "First, frame the problem in a way that makes the problem clear; the objective must be clarified to know what the problem is for. The objective is:\n",
        "1. to feed the machine learning system, the model's output.\n",
        "2. Other signals may be fed including the predictions.\n",
        "\n",
        "When the solution is implemented, it should be able to predict housing prices in California. Because currently, housing price estimation is done by a team of experts who will manually calculate it through a complexity of rules. In which, it can be expected to take a lot of time.\n",
        "\n",
        "By providing a new solution through the project, all the complicated rules and calculations can be done much faster and more efficiently.\n",
        "\n",
        "The design of the machine learning system will be based around supervised learning which is perfect for tasks such as predictions using regression. For the ease of the project, batch learning is the chosen learning method.\n",
        "\n",
        "As for the measurement of the performance of the system, it will be measured through a root mean square error calculation because it is the best equation for determining the performance of predictions through regression. It emphasizes differences in predictions and actual values more which is crucial in determining performance since for our objective, it is to predict housing prices in which large errors make a big difference. The minimum performance needed for the project to be accepted is somewhere wihtin the median of the total datasets for predictions.\n",
        "\n",
        "Other problems similar to the one in being done in the project are stock market predictions. It could definitely be applied to; because the task required to solve the problem is another revolving around regression. In the long term, the ML system could definitely be reused for other problems too. Since, Human expertise in the area is limited nor as efficient. Manually solving the problem would require more time as human resources need to be deployed, in other words, man power. The amount of data required is tremendous as many things are to be taken into account such as size, rooms, conditions, and etc.\n",
        "\n",
        "Assumptions to be made and checked is whether the system actually needs the exact prices or does it actually need the categories in which the prices will fall into. Another assumption made is that whatever is fed into the system is going to output as expected, in more specific terms, prices.\n",
        "\n",
        "Fortunately, the project is about housing prices, according to the lectures, so it would be no surprise for the project to be a regression task. As for the second assumption, it would have to be verified below as the project commences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exJVMkccyB5i",
      "metadata": {
        "id": "exJVMkccyB5i"
      },
      "source": [
        "### Getting the data\n",
        "\n",
        "The data needed for the predictions of housing prices in California can be downloaded and obtained inside a file called \"*housing.tgz*\" using the below code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iYEBd_pF6i2g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEBd_pF6i2g",
        "outputId": "606132ef-3bf9-4ad9-ba0e-231e04f8b2de"
      },
<<<<<<<<< Temporary merge branch 1
      "id": "iYEBd_pF6i2g",
      "execution_count": 43,
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "72470bb0-9c14-4fde-94f6-e6ce2be1ffab",
      "metadata": {
        "id": "72470bb0-9c14-4fde-94f6-e6ce2be1ffab"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "def load_housing_data():\n",
        "  tarball_path = Path(\"datasets/housing.tgz\")\n",
        "  if not tarball_path.is_file():\n",
        "    Path(\"datasets\").mkdir(parents=True,exist_ok=True)\n",
        "    url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "    urllib.request.urlretrieve(url,tarball_path)\n",
        "    with tarfile.open(tarball_path) as housing_tarball:\n",
        "      housing_tarball.extractall(path=\"datasets\")\n",
        "  return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MT0hbNYI1_GS",
      "metadata": {
        "id": "MT0hbNYI1_GS"
      },
      "source": [
        "#### Where to get it\n",
        "The data is obtained through github at ageron's repository called \"*handson-ml2*\". The function *load_housing_data* works in the following sequence:\n",
        "\n",
        " 1. First, it checks if the given file inside *tarball_path* exists.\n",
        " 2. If it exists; it will go to the return statement for returning a panda csv. if not, the if clause will be executed.\n",
        " 3. The if clause, in summary, will create a directory *datasets*. if it does not already exist; if the *exist_ok* argument is true then there will be no errors when it finds that *datasets* directory already exists. The *parent* argument gives permission to modify parent directories.\n",
        " 4. *urllib.request.urlretrieve* will download whatever is at *url* and download it to *tarball_path*.\n",
        " 5. Lastly, the zip folder will be extracted and returned.\n",
        "\n",
        " The zip file, *housing.tgz*, should be around 400 KB according to github's information at *https://github.com/ageron/handson-ml2/blob/master/datasets/housing/housing.tgz*.\n",
        "\n",
        "A lot of the data found in the California housing prices project should be open-source and free on the internet for anyone to download at *https://github.com/ageron/handson-ml2/tree/master/datasets*.\n",
        "\n",
        "Since the project is in Google colabs, a lot of the modules necessary for the project has already been pre-installed. Furthermore, the necessary space for the project to work on should be temporary. Once, the session is over; it is said that the workspace disappears.\n",
        "\n",
        "#### Taking a look at the data\n",
        "The code below should display first top five records of the project's data, *housing.tgz*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FGm7OZrNc4-J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FGm7OZrNc4-J",
        "outputId": "eb84cb9c-c62b-41be-f1c1-f4209b84352b"
      },
<<<<<<<<< Temporary merge branch 1
      "id": "FGm7OZrNc4-J",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -122.23     37.88                41.0        880.0           129.0   \n",
              "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
              "2    -122.24     37.85                52.0       1467.0           190.0   \n",
              "3    -122.25     37.85                52.0       1274.0           235.0   \n",
              "4    -122.25     37.85                52.0       1627.0           280.0   \n",
              "\n",
              "   population  households  median_income  median_house_value ocean_proximity  \n",
              "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
              "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
              "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
              "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
              "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b360e20-9d9a-4ac9-b40b-483072dcfff8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b360e20-9d9a-4ac9-b40b-483072dcfff8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b360e20-9d9a-4ac9-b40b-483072dcfff8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b360e20-9d9a-4ac9-b40b-483072dcfff8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cccb4f09-0e04-47ef-9a7d-2b01ae9fc040\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cccb4f09-0e04-47ef-9a7d-2b01ae9fc040')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cccb4f09-0e04-47ef-9a7d-2b01ae9fc040 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "housing",
              "summary": "{\n  \"name\": \"housing\",\n  \"rows\": 20640,\n  \"fields\": [\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.0035317235025802,\n        \"min\": -124.35,\n        \"max\": -114.31,\n        \"num_unique_values\": 844,\n        \"samples\": [\n          -123.39,\n          -122.08,\n          -116.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.135952397457107,\n        \"min\": 32.54,\n        \"max\": 41.95,\n        \"num_unique_values\": 862,\n        \"samples\": [\n          36.16,\n          36.28,\n          32.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing_median_age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.585557612111748,\n        \"min\": 1.0,\n        \"max\": 52.0,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          35.0,\n          25.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2181.6152515827785,\n        \"min\": 2.0,\n        \"max\": 39320.0,\n        \"num_unique_values\": 5926,\n        \"samples\": [\n          913.0,\n          1759.0,\n          6434.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_bedrooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 421.38507007403194,\n        \"min\": 1.0,\n        \"max\": 6445.0,\n        \"num_unique_values\": 1923,\n        \"samples\": [\n          1296.0,\n          3493.0,\n          2035.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1132.4621217653341,\n        \"min\": 3.0,\n        \"max\": 35682.0,\n        \"num_unique_values\": 3888,\n        \"samples\": [\n          3622.0,\n          3232.0,\n          2291.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"households\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 382.329752831612,\n        \"min\": 1.0,\n        \"max\": 6082.0,\n        \"num_unique_values\": 1815,\n        \"samples\": [\n          745.0,\n          1248.0,\n          1166.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8998217179452694,\n        \"min\": 0.4999,\n        \"max\": 15.0001,\n        \"num_unique_values\": 12928,\n        \"samples\": [\n          2.5211,\n          4.2454,\n          2.1681\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_house_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115395.61587441301,\n        \"min\": 14999.0,\n        \"max\": 500001.0,\n        \"num_unique_values\": 3842,\n        \"samples\": [\n          400700.0,\n          264900.0,\n          119100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocean_proximity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"<1H OCEAN\",\n          \"ISLAND\",\n          \"INLAND\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pEY8-9_fdxdq",
      "metadata": {
        "id": "pEY8-9_fdxdq"
      },
      "source": [
        "*head* is a method to display the top 5 rows of a data structure. Another useful method to see more information on the project's data structure and its data is *info* as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P_Ieh5fpeMp6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Ieh5fpeMp6",
        "outputId": "29a5d9c2-32a3-4064-e6fc-cd4f28d869c1"
      },
<<<<<<<<< Temporary merge branch 1
      "id": "P_Ieh5fpeMp6",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20640 entries, 0 to 20639\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           20640 non-null  float64\n",
            " 1   latitude            20640 non-null  float64\n",
            " 2   housing_median_age  20640 non-null  float64\n",
            " 3   total_rooms         20640 non-null  float64\n",
            " 4   total_bedrooms      20433 non-null  float64\n",
            " 5   population          20640 non-null  float64\n",
            " 6   households          20640 non-null  float64\n",
            " 7   median_income       20640 non-null  float64\n",
            " 8   median_house_value  20640 non-null  float64\n",
            " 9   ocean_proximity     20640 non-null  object \n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uVHFrhvsenv4",
      "metadata": {
        "id": "uVHFrhvsenv4"
      },
      "source": [
        "Finally, after taking a quick peek at the housing data, it could be seen that the housing data has a total of 10 columns, and a total of 20640 entries. All columns except for *ocean_proiximity* are numerical data. In fact, it is a categorical attribute. Hence, its values are unfamiliar to every observer. In order to see all values that exist for *ocean_proximity*; one can use the *value_counts* method to list all possible values as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bIXvgrHXgyKU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "bIXvgrHXgyKU",
        "outputId": "fce9e109-cc52-464c-9331-50db0803e5f4"
      },
<<<<<<<<< Temporary merge branch 1
      "id": "bIXvgrHXgyKU",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ocean_proximity\n",
              "<1H OCEAN     9136\n",
              "INLAND        6551\n",
              "NEAR OCEAN    2658\n",
              "NEAR BAY      2290\n",
              "ISLAND           5\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ocean_proximity</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>&lt;1H OCEAN</th>\n",
              "      <td>9136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INLAND</th>\n",
              "      <td>6551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NEAR OCEAN</th>\n",
              "      <td>2658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NEAR BAY</th>\n",
              "      <td>2290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISLAND</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_R5giRVhC6i",
      "metadata": {
        "id": "m_R5giRVhC6i"
      },
      "source": [
        "As for the numerical attributes, a summary of them could be viewed through a method called *describe*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NoLzfGyPhUK3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "NoLzfGyPhUK3",
        "outputId": "7ae0c915-a6d1-4bef-b961-9d23e0c728dc"
      },
<<<<<<<<< Temporary merge branch 1
      "id": "NoLzfGyPhUK3",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          longitude      latitude  housing_median_age   total_rooms  \\\n",
              "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
              "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
              "std        2.003532      2.135952           12.585558   2181.615252   \n",
              "min     -124.350000     32.540000            1.000000      2.000000   \n",
              "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
              "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
              "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
              "max     -114.310000     41.950000           52.000000  39320.000000   \n",
              "\n",
              "       total_bedrooms    population    households  median_income  \\\n",
              "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
              "mean       537.870553   1425.476744    499.539680       3.870671   \n",
              "std        421.385070   1132.462122    382.329753       1.899822   \n",
              "min          1.000000      3.000000      1.000000       0.499900   \n",
              "25%        296.000000    787.000000    280.000000       2.563400   \n",
              "50%        435.000000   1166.000000    409.000000       3.534800   \n",
              "75%        647.000000   1725.000000    605.000000       4.743250   \n",
              "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
              "\n",
              "       median_house_value  \n",
              "count        20640.000000  \n",
              "mean        206855.816909  \n",
              "std         115395.615874  \n",
              "min          14999.000000  \n",
              "25%         119600.000000  \n",
              "50%         179700.000000  \n",
              "75%         264725.000000  \n",
              "max         500001.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c719959-7e94-4223-a6d0-0f438913682e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20433.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "      <td>20640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-119.569704</td>\n",
              "      <td>35.631861</td>\n",
              "      <td>28.639486</td>\n",
              "      <td>2635.763081</td>\n",
              "      <td>537.870553</td>\n",
              "      <td>1425.476744</td>\n",
              "      <td>499.539680</td>\n",
              "      <td>3.870671</td>\n",
              "      <td>206855.816909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.003532</td>\n",
              "      <td>2.135952</td>\n",
              "      <td>12.585558</td>\n",
              "      <td>2181.615252</td>\n",
              "      <td>421.385070</td>\n",
              "      <td>1132.462122</td>\n",
              "      <td>382.329753</td>\n",
              "      <td>1.899822</td>\n",
              "      <td>115395.615874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-124.350000</td>\n",
              "      <td>32.540000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>14999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-121.800000</td>\n",
              "      <td>33.930000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1447.750000</td>\n",
              "      <td>296.000000</td>\n",
              "      <td>787.000000</td>\n",
              "      <td>280.000000</td>\n",
              "      <td>2.563400</td>\n",
              "      <td>119600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-118.490000</td>\n",
              "      <td>34.260000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>2127.000000</td>\n",
              "      <td>435.000000</td>\n",
              "      <td>1166.000000</td>\n",
              "      <td>409.000000</td>\n",
              "      <td>3.534800</td>\n",
              "      <td>179700.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-118.010000</td>\n",
              "      <td>37.710000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>3148.000000</td>\n",
              "      <td>647.000000</td>\n",
              "      <td>1725.000000</td>\n",
              "      <td>605.000000</td>\n",
              "      <td>4.743250</td>\n",
              "      <td>264725.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>-114.310000</td>\n",
              "      <td>41.950000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>39320.000000</td>\n",
              "      <td>6445.000000</td>\n",
              "      <td>35682.000000</td>\n",
              "      <td>6082.000000</td>\n",
              "      <td>15.000100</td>\n",
              "      <td>500001.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c719959-7e94-4223-a6d0-0f438913682e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c719959-7e94-4223-a6d0-0f438913682e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c719959-7e94-4223-a6d0-0f438913682e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0d373167-ad25-41fc-9544-4470f7ddfaa3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d373167-ad25-41fc-9544-4470f7ddfaa3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0d373167-ad25-41fc-9544-4470f7ddfaa3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"housing\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7333.554670164394,\n        \"min\": -124.35,\n        \"max\": 20640.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -119.56970445736432,\n          -118.49,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7286.333552413666,\n        \"min\": 2.135952397457107,\n        \"max\": 20640.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35.63186143410853,\n          34.26,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing_median_age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7288.35672120143,\n        \"min\": 1.0,\n        \"max\": 20640.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          28.639486434108527,\n          29.0,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13944.990983306394,\n        \"min\": 2.0,\n        \"max\": 39320.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2635.7630813953488,\n          2127.0,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_bedrooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7106.427031043755,\n        \"min\": 1.0,\n        \"max\": 20433.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          537.8705525375618,\n          435.0,\n          20433.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13192.258841737372,\n        \"min\": 3.0,\n        \"max\": 35682.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1425.4767441860465,\n          1166.0,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"households\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7167.532601135343,\n        \"min\": 1.0,\n        \"max\": 20640.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          499.5396802325581,\n          409.0,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7295.721435853639,\n        \"min\": 0.4999,\n        \"max\": 20640.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.8706710029069766,\n          3.5347999999999997,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median_house_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 156160.28379826449,\n        \"min\": 14999.0,\n        \"max\": 500001.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          206855.81690891474,\n          179700.0,\n          20640.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rS9NSBanhrfW",
      "metadata": {
        "id": "rS9NSBanhrfW"
      },
      "source": [
        "Another good way to see the data is to plot it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ss5kTr1Why6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "ss5kTr1Why6b",
        "outputId": "35eac3c0-6f76-4d93-f99b-2bfda79981f9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50,figsize=(12,8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d240p8L9H9TM",
      "metadata": {
        "id": "d240p8L9H9TM"
      },
      "source": [
        "#### Creating a test set\n",
        "The creation of a test set is simple; it involves the random picking of about 20% of the dataset, and setting them aside:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "o6PIgJhMJl43",
      "metadata": {
        "id": "o6PIgJhMJl43"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def shuffle_and_split_data(data,test_ratio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  test_set_size = int(len(data)*test_ratio)\n",
        "  test_indices = shuffled_indices[:test_set_size]\n",
        "  train_indices = shuffled_indices[test_set_size:]\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immPry9lO_AI",
      "metadata": {
        "id": "immPry9lO_AI"
      },
      "source": [
        "The *shuffle_and_split_data* method has 2 parameters, *data* and *test_ratio* which refers to the our dataset, and *test_ratio* refers to the percentage of our dataset that will be set aside. In the current scenario, *housing* will be put in the place of *data*, and *test_ratio*, in its place, will be *0.2* for 20 percent.\n",
        "\n",
        "*shuffle_and_split_data* works by first, using the numpy *random* module's *permutation* method to return a list of randomly ordered indices, starting from 0 to whatever number was used as input for the method. Then, total entries is multiplied by *test_ratio* to get twenty percent of the total data which is going to be used to separate the *test_indices* from the *train_indices*. Finally, returning them in a tuple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3dvAc5vnpSTx",
      "metadata": {
        "id": "3dvAc5vnpSTx"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = shuffle_and_split_data(housing,0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6Andce2pxEq",
      "metadata": {
        "id": "H6Andce2pxEq"
      },
      "source": [
        "Now to display the length of the sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5EXIX9c9p2VM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EXIX9c9p2VM",
        "outputId": "cde00157-7106-4c78-a368-0498ca17a6e7"
      },
      "outputs": [],
      "source": [
        "print(\"train set: \"+ str(len(train_set)))\n",
        "print(\"test set: \"+str(len(test_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eYL0FZ0IqNkE",
      "metadata": {
        "id": "eYL0FZ0IqNkE"
      },
      "source": [
        "Running the *shuffle_and_split_data* method again will give different sets. Note that seeing the whole dataset should be avoided which is why it should not be run too many times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Z8YzGquPxNFi",
      "metadata": {
        "id": "Z8YzGquPxNFi"
      },
      "outputs": [],
      "source": [
        "from zlib import crc32\n",
        "\n",
        "def is_id_in_test_set(identifier, test_ratio):\n",
        "  return crc32(np.int64(identifier)) < test_ratio *2**32\n",
        "\n",
        "def split_data_with_id_hash(data,test_ratio,id_column):\n",
        "  ids= data[id_column]\n",
        "  in_test_set = ids.apply(lambda id_:is_id_in_test_set(id_,test_ratio))\n",
        "  return data.loc[~in_test_set],data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y89cHJdsWZ8n",
      "metadata": {
        "id": "y89cHJdsWZ8n"
      },
      "source": [
        "The code above is a solution to the problem when running the program too many times. The solution is to use an identifier of an instance, and create a hash. The hash will will help determine if the instance belong in the test set or not by comparing the instance's hash to 20 percent of the mamximum hash value. If lower or equal to it then it may be put into the test set.\n",
        "\n",
        "The *is_id_in_test_set* method is a method that returns boolean values by comparing hashes as explained. *split_data_with_id_hash* will use the *is_id_in_test_set* method by:\n",
        " 1. Getting the the id column of the data to get the identifiers\n",
        " 2. Use the *is_id_in_test_set* method on each instances and putting the results in *in_test_set*.\n",
        " 3. Returns two lists of records, one not belonging in the testset and the other belonging.\n",
        "\n",
        "Since, the project doesn't seem to have an id column in it. An id column will have to be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "FffhVSnSa1Zm",
      "metadata": {
        "id": "FffhVSnSa1Zm"
      },
      "outputs": [],
      "source": [
        "housing_with_id = housing.reset_index()\n",
        "train_set,test_set = split_data_with_id_hash(housing_with_id,0.2,\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3pQ-TRgpbMS_",
      "metadata": {
        "id": "3pQ-TRgpbMS_"
      },
      "source": [
        "After that, split the data sets into subsets with scikit-learn's *train_test_split*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "h1fPMX0jcPjA",
      "metadata": {
        "id": "h1fPMX0jcPjA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5QcC5pQUdreh",
      "metadata": {
        "id": "5QcC5pQUdreh"
      },
      "source": [
        "Next, split it more into a bunch of strata to represent various groups of a population such as income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAEptxeYd2pa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "DAEptxeYd2pa",
        "outputId": "8ea1faad-1273-4a4f-cfce-56932ac20d3e"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],bins=[0,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])\n",
        "\n",
        "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0,grid=True)\n",
        "plt.xlabel(\"Income category\")\n",
        "plt.ylabel(\"Number of districts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "B8DBSITjf1Yg",
      "metadata": {
        "id": "B8DBSITjf1Yg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "splitter = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42)\n",
        "strat_splits = []\n",
        "for train_index,test_index in splitter.split(housing,housing[\"income_cat\"]):\n",
        "  strat_train_set_n = housing.iloc[train_index]\n",
        "  strat_test_set_n = housing.iloc[test_index]\n",
        "  strat_splits.append([strat_train_set_n,strat_test_set_n])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ySPjriLTg5-U",
      "metadata": {
        "id": "ySPjriLTg5-U"
      },
      "source": [
        "Split the test set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1X45Is-GhRJu",
      "metadata": {
        "id": "1X45Is-GhRJu"
      },
      "outputs": [],
      "source": [
        "strat_train_set,strat_test_set = strat_splits[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1a4Um2DhmKB",
      "metadata": {
        "id": "g1a4Um2DhmKB"
      },
      "source": [
        "Taking a look at the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1e7mTThlie",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "bd1e7mTThlie",
        "outputId": "b8f0ed41-131c-4bcc-ee3d-afa4d804031c"
      },
      "outputs": [],
      "source": [
        "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iGpgR_-kiPzZ",
      "metadata": {
        "id": "iGpgR_-kiPzZ"
      },
      "source": [
        "Since the *income_cat* column won't be used again. It could be dropped by doing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eXewE8hRiPi6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXewE8hRiPi6",
        "outputId": "3ca2dbf9-32e1-4632-9e19-d8fbb732384f"
      },
      "outputs": [],
      "source": [
        "\n",
        "for set_ in (strat_train_set,strat_test_set):\n",
        "  set_.drop(\"income_cat\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Isdt0Q_uuj2U",
      "metadata": {
        "id": "Isdt0Q_uuj2U"
      },
      "source": [
        "### Exploring the data\n",
        "As the previous section has covered, the data has 10 attribute; 9 of them being floats and the last one being objects, including the newly added column to *housing*, *income_cat*. The relationship seems to be linear and the target attribute might just be income. The reason being that the result has already been tested and shown by previous research, so if the current project is done correctly according to the right procedures. There shouldn't be much of a difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyWhZpvCxmRM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyWhZpvCxmRM",
        "outputId": "c6f3b617-97d0-4ae5-b223-153d1ec98628"
      },
      "outputs": [],
      "source": [
        "housing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5hC7UGRqR9KW",
      "metadata": {
        "id": "5hC7UGRqR9KW"
      },
      "source": [
        "The scatter plot for the current data is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZJPCneOiR87d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ZJPCneOiR87d",
        "outputId": "cdb67ef0-c7c5-4391-de5d-29a0155607af"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",grid=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GW5Nnb6XT2KV",
      "metadata": {
        "id": "GW5Nnb6XT2KV"
      },
      "source": [
        "Next, visualizing geograpical data, in the project, it will be visualized through a predefined color map called *jet*. In which, blue represents low values and red represents high values. The color of the dots represents price as the radius represents the population of a district."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ux98ytbdU8_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "ux98ytbdU8_z",
        "outputId": "f60d950f-5ecf-4d08-e316-cb7a970fc6b0"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",grid=True,s=housing[\"population\"]/100,label=\"population\",c=\"median_house_value\",cmap=\"jet\",colorbar=True,\n",
        "             legend=True,sharex=False,figsize=(10,7))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CdmlIUVkXCCy",
      "metadata": {
        "id": "CdmlIUVkXCCy"
      },
      "source": [
        "#### Correlation of data\n",
        "Looking for correlation is easy due to there already being a pre-defined method for printing correlation. The method is called corr and will show the correlation between the attributes and the median housing price or in other words, which attributes of the ten contributes more to the average pricing of houses in California."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tUcCIsz2XpDJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "tUcCIsz2XpDJ",
        "outputId": "766ab351-a222-4d98-85a2-e859212d0573"
      },
      "outputs": [],
      "source": [
        "housing = pd.get_dummies(housing, columns=['ocean_proximity'])\n",
        "\n",
        "\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uePjyBRlyUPj",
      "metadata": {
        "id": "uePjyBRlyUPj"
      },
      "source": [
        "The correlation method is done using the **standard correlation coefficient** method which has a correlation coefficient that ranges from -1 to 1. When it is -1, it means a strong negative correlation; when it is a 1, it means a strong positive correlation; when it is 0, it means a weak correlation or no linear correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHhk6trJ569H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "nHhk6trJ569H",
        "outputId": "2fdc9aa4-e20f-4b50-b735-a8f5173afafb"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DormCkDu6erV",
      "metadata": {
        "id": "DormCkDu6erV"
      },
      "source": [
        "Another way to check for correlation is to use the *scatter matrix* method which plots attributes with emphasis on their differences. Other variables or data to consider taking a look at are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e97sfO-Y7WXG",
      "metadata": {
        "id": "e97sfO-Y7WXG"
      },
      "outputs": [],
      "source": [
        "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
        "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
        "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DzlfHK_N7bww",
      "metadata": {
        "id": "DzlfHK_N7bww"
      },
      "source": [
        "Then use the *corr* method again to find the correlations of attributes to average housing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7j8C4vw7sTU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "V7j8C4vw7sTU",
        "outputId": "c463dc7f-9d7d-4d81-fb3e-b84eb59a5186"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRKsXWee9N8y",
      "metadata": {
        "id": "LRKsXWee9N8y"
      },
      "source": [
        "As could be seen, income is a bigger contributor to the average housing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "nNx8jPen9UF7",
      "metadata": {
        "id": "nNx8jPen9UF7"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ftiESTTj9vSl",
      "metadata": {
        "id": "ftiESTTj9vSl"
      },
      "source": [
        "#### Clean the data\n",
        "A new training set must be made in order to prepare the data for ML aglorithms. The first step is to clean the data set by 3 ways:\n",
        " 1. Getting rid of corresponding districts\n",
        " 2. Getting rid of whole attribute\n",
        " 3. imputation\n",
        "\n",
        "All three ways could be done easily through pre-defined methods. The main points of all three options are ways to clean data or more specifically, fix missing values. However, scikit provides an even easier way to fix missing values by having the *SimpleImputer* class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9025d7ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1 removing\n",
        "# the corresponding districts\n",
        "\n",
        "housing.drop(\"total_bedrooms\", axis=1)  # option 2 removing the whole attribute\n",
        "\n",
        "median = housing[\"total_bedrooms\"].median()  # option 3 Set the missing values\n",
        "housing[\"total_bedrooms\"].fillna(median, inplace=True) # to some value (zero,\n",
        "# the mean, the median, etc.). This is called imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "hAT9ihXR_EIh",
      "metadata": {
        "id": "hAT9ihXR_EIh"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "housing_num = housing.select_dtypes(include=[np.number])\n",
        "imputer.fit(housing_num)\n",
        "X = imputer.transform(housing_num)\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing_num.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y20FnmLLA8-y",
      "metadata": {
        "id": "y20FnmLLA8-y"
      },
      "source": [
        "#### Reflections\n",
        "If the project were to be done manually, it would take a much significant amount of time as the whole entire process up from the top to the bottom took a lot of steps. We would have to gather all the data, graph it, apply calculations such as correlations to get the correlation of the attributes to the median housing price. Then, in order to reuse the data, it must be copied and then cleaned as well. The process would take years to do if done manually.\n",
        "\n",
        "There are, however, some promising transformations that could be done to the data. One might be the z-score which provides good scaling in data. As for extra data that might be useful, we believe is the crime rates. The crime rates would be something I believe would be very crucial in determining the median house pricing as well.\n",
        "\n",
        "In conclusion, what we have learned so far is how to generate plots, how to use correlation, convenient tools that could be used for future data manipulation tools such as *drop*, and *dropna*. We also learned the process to look and analyze data as well. But, what could still be learned furthermore are the different tools, and their different usage in different contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ec9f77",
      "metadata": {},
      "source": [
        "### Handling Text and Categorical Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234b1dce",
      "metadata": {},
      "source": [
        "Up to this point we have only dealt with numerical attributes, but there are also many cases where the data will contain text attributes.\n",
        "\n",
        "For example, the ocean_proximity attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee2492d",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a10bf6",
      "metadata": {},
      "source": [
        "Since most machine learning algorithms prefer to work with numbers, let's first convert the text to numbers using Scikit-Learn's OrdinalEncoder class like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d132e62d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c6481c",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_cat_encoded[:8]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e6286b",
      "metadata": {},
      "source": [
        "It is possible to get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d86b61",
      "metadata": {},
      "outputs": [],
      "source": [
        "ordinal_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce231b69",
      "metadata": {},
      "source": [
        "One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is \"<1H OCEAN\" (and 0 otherwise), another attribute equal to 1 when the category is \"INLAND\" (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fcb039b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fc7f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b471b2",
      "metadata": {},
      "source": [
        "If you want to convert the sparse matrix to a (dense) NumPy array, just call the toarray() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d231a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_cat_1hot.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5294494",
      "metadata": {},
      "source": [
        "To drop some outliers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52673e7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "outlier_pred = isolation_forest.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82555e91",
      "metadata": {},
      "source": [
        "Alternatively, you can set sparse_output=False when creating the OneHotEncoder (note: the sparse hyperparameter was renamned to sparse_output in Scikit-Learn 1.2):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3a279d",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64cc771",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfeed3b7",
      "metadata": {},
      "source": [
        "The Pandas function called get_dummies() converts each categorical feature into a one-hot representation, with one binary feature per category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f05fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
        "pd.get_dummies(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c836b78",
      "metadata": {},
      "source": [
        "The advantage of OneHotEncoder is that it remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. Look what our trained cat_encoder outputs when we make it transform the same df_test (using transform(), not fit_transform()):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10619c7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder.transform(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d45817",
      "metadata": {},
      "source": [
        "Since get_dummies() saw only two categories, it output two columns, whereas OneHotEncoder output one column per learned category, in the right order. Moreover, if you feed get_dummies() a DataFrame containing an unknown category (e.g., \"<2H OCEAN\"), it will happily generate a column for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d35562",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
        "pd.get_dummies(df_test_unknown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4474bbc3",
      "metadata": {},
      "source": [
        "OneHotEncoder is smarter: it will detect the unknown category and raise an exception. If you prefer, you can set the handle_unknown hyperparameter to \"ignore\", in which case it will just represent the unknown category with zeros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b9a3cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder.handle_unknown = \"ignore\"\n",
        "cat_encoder.transform(df_test_unknown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce24c065",
      "metadata": {},
      "source": [
        "When you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the feature_names_in_ attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to transform() or predict()) has the same column names. Transformers also provide a get_feature_names_out() method that you can use to build a DataFrame around the transformers output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c345ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder.feature_names_in_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8d4811",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_encoder.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796597ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n",
        "                         columns=cat_encoder.get_feature_names_out(),\n",
        "                         index=df_test_unknown.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443c2399",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e2d8c5",
      "metadata": {},
      "source": [
        "#### Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72dd66e",
      "metadata": {},
      "source": [
        "Feature scaling is one of the most important transformations that will need to be applied to your data. Machine learning algorithms dont generally perform well when the input of numerical attributes have very different scales. This is the case for  housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n",
        "\n",
        "The two main ways to get the attributes to have the same scale are: min-max  scaling (also known as normalization) and standardization.\n",
        "\n",
        "Scikit-Learn provides a transformer called MinMaxScaler for min-max scaling. In our case, we will want to set the range from (-1, 1) since that is what neural networks work best with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2547977a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a452b7d",
      "metadata": {},
      "source": [
        "In a similar way, Scikit-Learn provides a transformer called StandardScaler for standardization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9cc904b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc5a12eb",
      "metadata": {},
      "source": [
        "When a features distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally dont like this at all. Before you scaling the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical.\n",
        "\n",
        "For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help.\n",
        "\n",
        "For example, the population feature roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Figure 2-17 shows how much better this feature looks when you compute its log: its very close to a Gaussian distribution (i.e., bell-shaped)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d2f55f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extra code  this cell generates Figure 217\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
        "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
        "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
        "axs[0].set_xlabel(\"Population\")\n",
        "axs[1].set_xlabel(\"Log of population\")\n",
        "axs[0].set_ylabel(\"Number of districts\")\n",
        "save_fig(\"long_tail_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c4a35b",
      "metadata": {},
      "source": [
        "Another approach to handling heavy-tailed features is bucketizing the feature. This is especially the case if the feature has a multimodal distribution, such as the housing_median_age feature. Another approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF) like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5d26d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284b0ee2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extra code  this cell generates Figure 218\n",
        "\n",
        "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
        "                   housing[\"housing_median_age\"].max(),\n",
        "                   500).reshape(-1, 1)\n",
        "gamma1 = 0.1\n",
        "gamma2 = 0.03\n",
        "rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\n",
        "rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel(\"Housing median age\")\n",
        "ax1.set_ylabel(\"Number of districts\")\n",
        "ax1.hist(housing[\"housing_median_age\"], bins=50)\n",
        "\n",
        "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
        "color = \"blue\"\n",
        "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
        "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "ax2.set_ylabel(\"Age similarity\", color=color)\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "save_fig(\"age_similarity_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3561a5f1",
      "metadata": {},
      "source": [
        "There are also helpful tools to help transform the target values. One way is to use the LinearRegression class from Scikit-Learn like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6cf4e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "target_scaler = StandardScaler()\n",
        "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
        "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
        "\n",
        "scaled_predictions = model.predict(some_new_data)\n",
        "predictions = target_scaler.inverse_transform(scaled_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df08a24",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53c4e7d3",
      "metadata": {},
      "source": [
        "There is a simpler way of doing this is which is to use Scikit-Learn's TransformedTargetRegressor class as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8999d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "model = TransformedTargetRegressor(LinearRegression(),\n",
        "                                   transformer=StandardScaler())\n",
        "model.fit(housing[[\"median_income\"]], housing_labels)\n",
        "predictions = model.predict(some_new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf8afec",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21cf5bce",
      "metadata": {},
      "source": [
        "### Custom Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32b73a6",
      "metadata": {},
      "source": [
        "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom transformations, cleanup operations, or combining specific attributes.\n",
        "\n",
        "For example, creating a log-transformer and applying it to the population feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9787e635",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
        "log_pop = log_transformer.transform(housing[[\"population\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8512193a",
      "metadata": {},
      "source": [
        "Heres a transformer that computes the same Gaussian RBF similarity measure as earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d253a3fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
        "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d120aa98",
      "metadata": {},
      "outputs": [],
      "source": [
        "age_simil_35"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5980ec5",
      "metadata": {},
      "source": [
        "Adding a feature that will measure the geographic similarity between each district and San Francisco:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc47652",
      "metadata": {},
      "outputs": [],
      "source": [
        "sf_coords = 37.7749, -122.41\n",
        "sf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\n",
        "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202ae9be",
      "metadata": {},
      "outputs": [],
      "source": [
        "sf_simil"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9101bdd",
      "metadata": {},
      "source": [
        "Custom transformers are also able to combine features, like FunctionTransformer that computes the ratio between the input features 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946b813c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
        "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67619c48",
      "metadata": {},
      "source": [
        "To make FunctionTransformer trainable you would need to create a custom class that has these three methods: fit() (which must return self), transform(), and fit_transform().\n",
        "\n",
        "As an example, here's a custom transformer class that acts similarly to StandardScaler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fabb743",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_array, check_is_fitted\n",
        "\n",
        "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
        "        self.with_mean = with_mean\n",
        "\n",
        "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
        "        X = check_array(X)  # checks that X is an array with finite float values\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        self.scale_ = X.std(axis=0)\n",
        "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
        "        X = check_array(X)\n",
        "        assert self.n_features_in_ == X.shape[1]\n",
        "        if self.with_mean:\n",
        "            X = X - self.mean_\n",
        "        return X / self.scale_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0101ebac",
      "metadata": {},
      "source": [
        "A custom transformer can (and often does) use other estimators in its implementation. Here is an example of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c4b6f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.gamma = gamma\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
        "                              random_state=self.random_state)\n",
        "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
        "\n",
        "    def get_feature_names_out(self, names=None):\n",
        "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4097e1fa",
      "metadata": {},
      "source": [
        "Putting this custom transformer into use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d33212",
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
        "                                           sample_weight=housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19d496e",
      "metadata": {},
      "source": [
        "This creates a ClusterSimilarity transformer with 10 clusters. To look at the first three rows while rounding to two decimal places we call the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85f231f",
      "metadata": {},
      "outputs": [],
      "source": [
        "similarities[:3].round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76153137",
      "metadata": {},
      "source": [
        "The following shows the 10 clusters colored according to their geographic similarity to their closest cluster center:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e83076",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extra code  this cell generates Figure 219\n",
        "\n",
        "housing_renamed = housing.rename(columns={\n",
        "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
        "    \"population\": \"Population\",\n",
        "    \"median_house_value\": \"Median house value (s)\"})\n",
        "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
        "\n",
        "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
        "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
        "                     c=\"Max cluster similarity\",\n",
        "                     cmap=\"jet\", colorbar=True,\n",
        "                     legend=True, sharex=False, figsize=(10, 7))\n",
        "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
        "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
        "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
        "         label=\"Cluster centers\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "save_fig(\"district_cluster_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5e6f386",
      "metadata": {},
      "source": [
        "#### Transformation Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9869ac46",
      "metadata": {},
      "source": [
        "To better facilitate the data transformation steps Scikit-Learn has the Pipeline class. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d10344",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"standardize\", StandardScaler()),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95373e9f",
      "metadata": {},
      "source": [
        "There is also the option to use the make_pipeline() function instead; it takes transformers as positional arguments and creates a Pipeline using the names of the transformers classes, in lowercase and without underscores (e.g., \"simpleimputer\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8443556c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ac51d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "set_config(display='diagram')\n",
        "\n",
        "num_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5a1505",
      "metadata": {},
      "source": [
        "Calling the pipelines fit_transform() method and looking at the outputs first two rows, rounded to two decimal places:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc55c085",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
        "housing_num_prepared[:2].round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c4a541",
      "metadata": {},
      "source": [
        "Calling the pipelines get_feature_names_out() method to recover a nice DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6c3de7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_housing_num_prepared = pd.DataFrame(\n",
        "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
        "    index=housing_num.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e420b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_housing_num_prepared.head(2)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8e4c6d",
      "metadata": {},
      "source": [
        "Method to have a single transformer that is capable of handling all columns and applying the appropriate transformations on each column using the ColumnTransformer class from Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23efced",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
        "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "cat_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", cat_pipeline, cat_attribs),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10636bc",
      "metadata": {},
      "source": [
        "Passing make_column_selector to make_column_transformer to automatically name the transformer and select all features of a given type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a094fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "\n",
        "preprocessing = make_column_transformer(\n",
        "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
        "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba08cb1",
      "metadata": {},
      "source": [
        "Applying ColumnTransformer to the housing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a613943",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f63534",
      "metadata": {},
      "source": [
        "Creating a single pipeline that will perform all the transformations that have been experimented with up to now. The following code builds the pipeline to do all of this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e9ad85",
      "metadata": {},
      "outputs": [],
      "source": [
        "def column_ratio(X):\n",
        "    return X[:, [0]] / X[:, [1]]\n",
        "\n",
        "def ratio_name(function_transformer, feature_names_in):\n",
        "    return [\"ratio\"]  # feature names out\n",
        "\n",
        "def ratio_pipeline():\n",
        "    return make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
        "        StandardScaler())\n",
        "\n",
        "log_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
        "    StandardScaler())\n",
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
        "                                     StandardScaler())\n",
        "preprocessing = ColumnTransformer([\n",
        "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
        "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
        "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
        "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
        "                               \"households\", \"median_income\"]),\n",
        "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
        "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
        "    ],\n",
        "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e5cbec",
      "metadata": {},
      "source": [
        "Testing the functionality of the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d982158d",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)\n",
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a941cf7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessing.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14778930",
      "metadata": {},
      "source": [
        "### Select and Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d02720b",
      "metadata": {},
      "source": [
        "#### Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05eb3ad5",
      "metadata": {},
      "source": [
        "Starting off with training a basic linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29cfa4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
        "lin_reg.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e402971a",
      "metadata": {},
      "source": [
        "Testing the linear regression model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53802338",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_predictions = lin_reg.predict(housing)\n",
        "housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9a1607",
      "metadata": {},
      "source": [
        "And comparing against the actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81262910",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_labels.iloc[:5].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ebba66",
      "metadata": {},
      "source": [
        "Measuring this regression models RMSE on the whole training set using Scikit-Learns mean_squared_error() function, with the squared argument set to False:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6dc1907",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                              squared=False)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f23ec5",
      "metadata": {},
      "source": [
        "Since the model isn't working as well as what was hoped for we need to go about changing something. The three general options are: selecting a more powerful model, feeding the training algorithm with better features, or reducing the constraints on the model.\n",
        "\n",
        "The last option isn't realistic since the model isn't regularized, so we will go with the option of selecting a more powerful model. That model will be the DecisionTreeRegressor model as shown in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fe8308",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
        "tree_reg.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cabee5",
      "metadata": {},
      "source": [
        "Testing the newly trained DecisionTreeRegressor model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5dde595",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_predictions = tree_reg.predict(housing)\n",
        "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                              squared=False)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69355c2",
      "metadata": {},
      "source": [
        "Before finalizing anything or changing the test set, we will move onto model validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc96ad2",
      "metadata": {},
      "source": [
        "#### Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dc913f",
      "metadata": {},
      "source": [
        "This step of this process is about verifying whether the DecisionTreeRegressor model is correctly fit for the expected data.\n",
        "\n",
        "One great way to test that is to use Scikit-Learns k_-fold cross-validation feature. The following code will test and evaluate the model 10 different times and give an array containing the 10 evaluation scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40994c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2916ca08",
      "metadata": {},
      "source": [
        "Showing the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05832e88",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.Series(tree_rmses).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5974b65",
      "metadata": {},
      "source": [
        "Showcasing the error stats for the LinearRegression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a5fe139",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extra code  computes the error stats for the linear model\n",
        "lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
        "pd.Series(lin_rmses).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e6db6b",
      "metadata": {},
      "source": [
        "After doing this cross-validation, it can be seen that the DecisionTreeRegressor model performs similarly to the LinearRegression model. Although the process of cross-validation can provide you will valuable information about the model, sometimes it isn't always realistic because you will then need to train the model several times.\n",
        "\n",
        "The last model that we will be trying is the RandomForestRegressor model and the code is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c11f53",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = make_pipeline(preprocessing,\n",
        "                           RandomForestRegressor(random_state=42))\n",
        "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
        "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9778f745",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.Series(forest_rmses).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d62d5a8",
      "metadata": {},
      "source": [
        "Showcasing the RMSE for the RandomForestRegressor model on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99088c95",
      "metadata": {},
      "outputs": [],
      "source": [
        "forest_reg.fit(housing, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing)\n",
        "forest_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                                 squared=False)\n",
        "forest_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bada68c",
      "metadata": {},
      "source": [
        "After testing the new RandomForestRegressor model, it can be seen that it is much more accurate than what the previous two were, but it can also be seen that there is still a lot of overfitting as shown by the RMSE.\n",
        "\n",
        "Some potential solutions are to simplify the model, constrain the model, or to get more training data. This is where it comes down to shortlisting a few promising models after testing models from various categories of machine learning algorithms without worrying too much about the hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bf9c8a",
      "metadata": {},
      "source": [
        "# Fine Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afec804",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Once we get the shortlist that it value is looking promising to use, we will need to set the best fine tune that will give us the best result. One option is to fiddle around with the hyperparameter manually. However, this proves it can be tedious work since there are multiple way. To reduce this, there are a couple of ways we can change the hyperparameter setting. The first way is Grid search. What it does is it will use cross-validation to evaluayed all the possible combination of hyperameter values. Here an example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa60f94",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
        "])\n",
        "param_grid = [\n",
        "    {'preprocessing__geo__n_clusters': [5, 8, 10],\n",
        "     'random_forest__max_features': [4, 6, 8]},\n",
        "    {'preprocessing__geo__n_clusters': [10, 15],\n",
        "     'random_forest__max_features': [6, 8, 10]},\n",
        "]\n",
        "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
        "                           scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa0504e",
      "metadata": {},
      "source": [
        "In this code, we have two dictionaries in the param_grid values. The grid Serach will evaluate the number of columns there are (in this case there are 3) so we do 3 x 3 = 9 combination of n_cluster and the other dictionaries will be 2 x 3 = 6 which it the combination of hyperparameter value. In total we get 15 (n_cluster + hyperparameter value) combination of hyperparameter value. The grid search does 3-fold cross validation. What this means is the new total is 45 (3x15) which means that it does 45 rounds of training. This will take a while but once it done then run the program\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f976148e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "cv_res.head()  # note: the 1st column is the row ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29de4c85",
      "metadata": {},
      "source": [
        "We will get the mean test score. We be focusing on is the mean_test_rmse. The lower the score the better the model will perform.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b9c42d",
      "metadata": {},
      "source": [
        "The other is randomizing search is often better than the grid search. When it comes to hyperparameter search space is large. It has it benefit like\n",
        "if your hyperparameters are continuous or discrete and need to do 1000 iteration, then it will explore 1000 different values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcfcce7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
        "                  'random_forest__max_features': randint(low=2, high=20)}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(\n",
        "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
        "    scoring='neg_root_mean_squared_error', random_state=42)\n",
        "\n",
        "rnd_search.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b550b697",
      "metadata": {},
      "source": [
        "#### Ensemble Method\n",
        "\n",
        "\n",
        "\n",
        "Another way we can do is try to combine the models that perform the best. This is often better than the best Indvidual model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec630086",
      "metadata": {},
      "source": [
        "# Summary\n",
<<<<<<<<< Temporary merge branch 1
        "\n",
        "In conclusion, to make machine learning we must consider the following checklist. They are framing the problem, get data, explore the data, prepare the data for machine learning to use, explore different model, get the shortlist that is the best one, fine-tune the model to a great solution, present your model, and finally launch your system. They are important when it comes to making the model. For instance, we need to decide what kind of problem we are dealing with, like making a model of housing prices in state. For this, we need to get a lot of data that can help us make better predictions for the model to make. When getting data, we need to remove the one that isnt what we are looking for so that the model can be accurate. For instance, we remove data from the corresponding district, removing whole attribute, and removing value that is missing. Cleaning the data will allow us to fine fiddle around the hyperparameters. Grid search and random search allow us to fiddle around quickly. However, random search is best since it is faster finding the best fir model than gride search. Testing the result allows us to see if it the data return of what we expected, if it does then we can launch it."
      ],
      "metadata": {
        "id": "jjj3P-FfOY7G"
      },
      "id": "jjj3P-FfOY7G"
=========
        "\n"
      ]
>>>>>>>>> Temporary merge branch 2
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
<<<<<<<<< Temporary merge branch 1
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
=========
>>>>>>>>> Temporary merge branch 2
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
