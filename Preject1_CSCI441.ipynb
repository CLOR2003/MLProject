{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4df3253",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CLOR2003/MLProject/blob/Lor/Copy_of_Preject1_CSCI441.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07",
      "metadata": {
        "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07"
      },
      "source": [
        "# Project 1\n",
        "## CSCI 441\n",
        "## Group 2\n",
        "## Members: Chakong Lor, Zachary Sunder, Luis Aguilar\n",
        "## Date: 10/4/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6nKs_XSD3T5N",
      "metadata": {
        "id": "6nKs_XSD3T5N"
      },
      "source": [
        "# *TEMPORARY CELL\n",
        "### All prompts NEED to be answered before completion is acknowledged\n",
        "\n",
        "Questions to be answered for framing the problem:\n",
        "1. Define the objective in business terms.x\n",
        "2. How will your solution be used?x\n",
        "3. What are the current solutions/workarounds (if any)?x\n",
        "4. How should you frame this problem (supervised/unsupervised,\n",
        " online/offline, etc.)?x\n",
        "5. How should performance be measured?x\n",
        "6. Is the performance measure aligned with the business objective?x\n",
        "7. What would be the minimum performance needed to reach the\n",
        " business objective?x\n",
        "8. What are comparable problems? Can you reuse experience or\n",
        " tools?x\n",
        "9. Is human expertise available?x\n",
        "10. How would you solve the problem manually?x\n",
        "11. List the assumptions you (or others) have made so far.x\n",
        "12. Verify assumptions if possiblex\n",
        "\n",
        "Questions to be answered for Get the data:\n",
        " Note: automate as much as possible so you can easily get fresh data.\n",
        "1. List the data you need and how much you need.x\n",
        "2. Find and document where you can get that data.x\n",
        "3. Check how much space it will take.x\n",
        "4. Check legal obligations, and get authorization if necessary.x\n",
        "5. Get access authorizations.x\n",
        "6. Create a workspace (with enough storage space).x\n",
        "7. Get the data.x\n",
        "8. Convert the data to a format you can easily manipulate (without\n",
        " changing the data itself).x\n",
        "9. Ensure sensitive information is deleted or protected (e.g.,\n",
        " anonymized).x\n",
        "10. Check the size and type of data (time series, sample,\n",
        " geographical, etc.).x\n",
        "11. Sample a test set, put it aside, and never look at it (no data\n",
        " snooping!)x\n",
        "\n",
        "Questions to be answered for Explore the Data:\n",
        " Note: try to get insights from a field expert for these steps.\n",
        "1. Create a copy of the data for exploration (sampling it down to a\n",
        " manageable size if necessary).x\n",
        "2. Create a Jupyter notebook to keep a record of your data\n",
        " exploration.x\n",
        "3. Study each attribute and its characteristics:\n",
        " Name\n",
        " Type (categorical, int/float, bounded/unbounded, text,\n",
        " structured, etc.)\n",
        " % of missing values\n",
        " Noisiness and type of noise (stochastic, outliers,\n",
        " rounding errors, etc.)\n",
        " Usefulness for the task\n",
        " Type of distribution (Gaussian, uniform, logarithmic,\n",
        " etc.)x\n",
        "4. For supervised learning tasks, identify the target attribute(s).x\n",
        "5. Visualize the data.x\n",
        "6. Study the correlations between attributes.x\n",
        "7. Study how you would solve the problem manually.x\n",
        "8. Identify the promising transformations you may want to apply.x\n",
        "9. Identify extra data that would be useful (go back to “Get the\n",
        " Data”).x\n",
        "10. Document what you have learned.x\n",
        "\n",
        "Questions to be answered for Prepare the Data: Zachary\n",
        " * Work on copies of the data (keep the original dataset intact).\n",
        " * Write functions for all data transformations you apply, for five\n",
        " reasons:\n",
        " * So you can easily prepare the data the next time you get a\n",
        " fresh dataset\n",
        "  * So you can apply these transformations in future projects\n",
        "  * To clean and prepare the test set\n",
        "  * To clean and prepare new data instances once your solution is live\n",
        "  * To make it easy to treat your preparation choices as\n",
        " hyperparameters\n",
        " 1. Data cleaning:\n",
        " Fix or remove outliers (optional).\n",
        " Fill in missing values (e.g., with zero, mean, median…)\n",
        " or drop their rows (or columns).\n",
        " 2. Feature selection (optional):\n",
        " Drop the attributes that provide no useful information for\n",
        " the task.\n",
        " 3. Feature engineering, where appropriate:\n",
        " Discretize continuous features.\n",
        " Decompose features (e.g., categorical, date/time, etc.).\n",
        " 2\n",
        " Add promising transformations of features (e.g., log(x),\n",
        " sqrt(x), x , etc.).\n",
        " Aggregate features into promising new features.\n",
        " 4. Feature scaling: Standardize or normalize features\n",
        "\n",
        "Shortlist Promising Models: Zachary\n",
        " * If the data is huge, you may want to sample smaller training sets\n",
        " so you can train many different models in a reasonable time (be\n",
        " aware that this penalizes complex models such as large neural\n",
        " nets or Random Forests).\n",
        " * Once again, try to automate these steps as much as possible.\n",
        " 1. Train many quick-and-dirty models from different categories\n",
        " (e.g., linear, naive Bayes, SVM, Random Forest, neural net, etc.)\n",
        " using standard parameters.\n",
        " 2. Measure and compare their performance.\n",
        " * For each model, use N-fold cross-validation and compute\n",
        " the mean and standard deviation of the performance\n",
        " measure on the N folds.\n",
        " 3. Analyze the most significant variables for each algorithm.\n",
        " 4. Analyze the types of errors the models make.\n",
        "What data would a human have used to avoid these\n",
        " errors?\n",
        " 5. Perform a quick round of feature selection and engineering.\n",
        " 6. Perform one or two more quick iterations of the five previous\n",
        " steps.\n",
        " 7. Shortlist the top three to five most promising models, preferring\n",
        " models that make different types of errors.\n",
        "\n",
        "Fine-Tune the System: Luis\n",
        "* You will want to use as much data as possible for this step,\n",
        " especially as you move toward the end of fine-tuning.\n",
        "* As always, automate what you can.\n",
        "1. Fine-tune the hyperparameters using cross-validation:\n",
        " * Treat your data transformation choices as\n",
        " hyperparameters, especially when you are not sure about\n",
        " them (e.g., if you’re not sure whether to replace missing\n",
        " values with zeros or with the median value, or to just\n",
        " drop the rows).\n",
        " * Unless there are very few hyperparameter values to\n",
        " explore, prefer random search over grid search. If\n",
        " training is very long, you may prefer a Bayesian\n",
        " optimization approach (e.g., using Gaussian process\n",
        " priors, as described by Jasper Snoek et al.).\n",
        " 1\n",
        "2. Try Ensemble methods. Combining your best models will often\n",
        " produce better performance than running them individually.\n",
        "3. Once you are confident about your final model, measure its\n",
        " performance on the test set to estimate the generalization error.\n",
        "\n",
        "Present Your Solution: Luis\n",
        " 1. Document what you have done.\n",
        " 2. Create a nice presentation.\n",
        " Make sure you highlight the big picture first.\n",
        " 3. Explain why your solution achieves the business objective.\n",
        " 4. Don’t forget to present interesting points you noticed along the\n",
        " way.\n",
        " Describe what worked and what did not.\n",
        " List your assumptions and your system’s limitations.\n",
        " 5. Ensure your key findings are communicated through beautiful\n",
        " visualizations or easy-to-remember statements (e.g., “the median\n",
        " income is the number-one predictor of housing prices”).\n",
        "\n",
        "Lastly: Luis\\\n",
        "Remember to write a summary at the bottom as to what we have learned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef472cdd-a7da-49c2-88c7-b3d8cfa56fdc",
      "metadata": {
        "id": "ef472cdd-a7da-49c2-88c7-b3d8cfa56fdc"
      },
      "source": [
        "### Frame the Problem and Look at the Big Picture\n",
        "\n",
        "First, frame the problem in a way that makes the problem clear; the objective must be clarified to know what the problem is for. The objective is:\n",
        "1. to feed the machine learning system, the model's output.\n",
        "2. Other signals may be fed including the predictions.\n",
        "\n",
        "When the solution is implemented, it should be able to predict housing prices in California. Because currently, housing price estimation is done by a team of experts who will manually calculate it through a complexity of rules. In which, it can be expected to take a lot of time.\n",
        "\n",
        "By providing a new solution through the project, all the complicated rules and calculations can be done much faster and more efficiently.\n",
        "\n",
        "The design of the machine learning system will be based around supervised learning which is perfect for tasks such as predictions using regression. For the ease of the project, batch learning is the chosen learning method.\n",
        "\n",
        "As for the measurement of the performance of the system, it will be measured through a root mean square error calculation because it is the best equation for determining the performance of predictions through regression. It emphasizes differences in predictions and actual values more which is crucial in determining performance since for our objective, it is to predict housing prices in which large errors make a big difference. The minimum performance needed for the project to be accepted is somewhere wihtin the median of the total datasets for predictions.\n",
        "\n",
        "Other problems similar to the one in being done in the project are stock market predictions. It could definitely be applied to; because the task required to solve the problem is another revolving around regression. In the long term, the ML system could definitely be reused for other problems too. Since, Human expertise in the area is limited nor as efficient. Manually solving the problem would require more time as human resources need to be deployed, in other words, man power. The amount of data required is tremendous as many things are to be taken into account such as size, rooms, conditions, and etc.\n",
        "\n",
        "Assumptions to be made and checked is whether the system actually needs the exact prices or does it actually need the categories in which the prices will fall into. Another assumption made is that whatever is fed into the system is going to output as expected, in more specific terms, prices.\n",
        "\n",
        "Fortunately, the project is about housing prices, according to the lectures, so it would be no surprise for the project to be a regression task. As for the second assumption, it would have to be verified below as the project commences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exJVMkccyB5i",
      "metadata": {
        "id": "exJVMkccyB5i"
      },
      "source": [
        "### Getting the data\n",
        "\n",
        "The data needed for the predictions of housing prices in California can be downloaded and obtained inside a file called \"*housing.tgz*\" using the below code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iYEBd_pF6i2g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEBd_pF6i2g",
        "outputId": "606132ef-3bf9-4ad9-ba0e-231e04f8b2de"
      },
      "outputs": [],
      "source": [
        "!pip install pathlib # If pathlib isn't pre-installed then run this statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "72470bb0-9c14-4fde-94f6-e6ce2be1ffab",
      "metadata": {
        "id": "72470bb0-9c14-4fde-94f6-e6ce2be1ffab"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "def load_housing_data():\n",
        "  tarball_path = Path(\"datasets/housing.tgz\")\n",
        "  if not tarball_path.is_file():\n",
        "    Path(\"datasets\").mkdir(parents=True,exist_ok=True)\n",
        "    url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "    urllib.request.urlretrieve(url,tarball_path)\n",
        "    with tarfile.open(tarball_path) as housing_tarball:\n",
        "      housing_tarball.extractall(path=\"datasets\")\n",
        "  return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MT0hbNYI1_GS",
      "metadata": {
        "id": "MT0hbNYI1_GS"
      },
      "source": [
        "#### Where to get it\n",
        "The data is obtained through github at ageron's repository called \"*handson-ml2*\". The function *load_housing_data* works in the following sequence:\n",
        "\n",
        " 1. First, it checks if the given file inside *tarball_path* exists.\n",
        " 2. If it exists; it will go to the return statement for returning a panda csv. if not, the if clause will be executed.\n",
        " 3. The if clause, in summary, will create a directory *datasets*. if it does not already exist; if the *exist_ok* argument is true then there will be no errors when it finds that *datasets* directory already exists. The *parent* argument gives permission to modify parent directories.\n",
        " 4. *urllib.request.urlretrieve* will download whatever is at *url* and download it to *tarball_path*.\n",
        " 5. Lastly, the zip folder will be extracted and returned.\n",
        "\n",
        " The zip file, *housing.tgz*, should be around 400 KB according to github's information at *https://github.com/ageron/handson-ml2/blob/master/datasets/housing/housing.tgz*.\n",
        "\n",
        "A lot of the data found in the California housing prices project should be open-source and free on the internet for anyone to download at *https://github.com/ageron/handson-ml2/tree/master/datasets*.\n",
        "\n",
        "Since the project is in Google colabs, a lot of the modules necessary for the project has already been pre-installed. Furthermore, the necessary space for the project to work on should be temporary. Once, the session is over; it is said that the workspace disappears.\n",
        "\n",
        "#### Taking a look at the data\n",
        "The code below should display first top five records of the project's data, *housing.tgz*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FGm7OZrNc4-J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FGm7OZrNc4-J",
        "outputId": "1c107cd0-4e9e-4ff7-eaf1-dcbbf62b946e"
      },
      "outputs": [],
      "source": [
        "housing.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pEY8-9_fdxdq",
      "metadata": {
        "id": "pEY8-9_fdxdq"
      },
      "source": [
        "*head* is a method to display the top 5 rows of a data structure. Another useful method to see more information on the project's data structure and its data is *info* as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P_Ieh5fpeMp6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Ieh5fpeMp6",
        "outputId": "29a5d9c2-32a3-4064-e6fc-cd4f28d869c1"
      },
      "outputs": [],
      "source": [
        "housing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uVHFrhvsenv4",
      "metadata": {
        "id": "uVHFrhvsenv4"
      },
      "source": [
        "Finally, after taking a quick peek at the housing data, it could be seen that the housing data has a total of 10 columns, and a total of 20640 entries. All columns except for *ocean_proiximity* are numerical data. In fact, it is a categorical attribute. Hence, its values are unfamiliar to every observer. In order to see all values that exist for *ocean_proximity*; one can use the *value_counts* method to list all possible values as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bIXvgrHXgyKU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "bIXvgrHXgyKU",
        "outputId": "fce9e109-cc52-464c-9331-50db0803e5f4"
      },
      "outputs": [],
      "source": [
        "housing['ocean_proximity'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_R5giRVhC6i",
      "metadata": {
        "id": "m_R5giRVhC6i"
      },
      "source": [
        "As for the numerical attributes, a summary of them could be viewed through a method called *describe*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NoLzfGyPhUK3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "NoLzfGyPhUK3",
        "outputId": "523adff2-3568-4d87-febe-8ccabad0bbea"
      },
      "outputs": [],
      "source": [
        "housing.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rS9NSBanhrfW",
      "metadata": {
        "id": "rS9NSBanhrfW"
      },
      "source": [
        "Another good way to see the data is to plot it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ss5kTr1Why6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "ss5kTr1Why6b",
        "outputId": "35eac3c0-6f76-4d93-f99b-2bfda79981f9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50,figsize=(12,8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d240p8L9H9TM",
      "metadata": {
        "id": "d240p8L9H9TM"
      },
      "source": [
        "#### Creating a test set\n",
        "The creation of a test set is simple; it involves the random picking of about 20% of the dataset, and setting them aside:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "o6PIgJhMJl43",
      "metadata": {
        "id": "o6PIgJhMJl43"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def shuffle_and_split_data(data,test_ratio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  test_set_size = int(len(data)*test_ratio)\n",
        "  test_indices = shuffled_indices[:test_set_size]\n",
        "  train_indices = shuffled_indices[test_set_size:]\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immPry9lO_AI",
      "metadata": {
        "id": "immPry9lO_AI"
      },
      "source": [
        "The *shuffle_and_split_data* method has 2 parameters, *data* and *test_ratio* which refers to the our dataset, and *test_ratio* refers to the percentage of our dataset that will be set aside. In the current scenario, *housing* will be put in the place of *data*, and *test_ratio*, in its place, will be *0.2* for 20 percent.\n",
        "\n",
        "*shuffle_and_split_data* works by first, using the numpy *random* module's *permutation* method to return a list of randomly ordered indices, starting from 0 to whatever number was used as input for the method. Then, total entries is multiplied by *test_ratio* to get twenty percent of the total data which is going to be used to separate the *test_indices* from the *train_indices*. Finally, returning them in a tuple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3dvAc5vnpSTx",
      "metadata": {
        "id": "3dvAc5vnpSTx"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = shuffle_and_split_data(housing,0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6Andce2pxEq",
      "metadata": {
        "id": "H6Andce2pxEq"
      },
      "source": [
        "Now to display the length of the sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5EXIX9c9p2VM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EXIX9c9p2VM",
        "outputId": "cde00157-7106-4c78-a368-0498ca17a6e7"
      },
      "outputs": [],
      "source": [
        "print(\"train set: \"+ str(len(train_set)))\n",
        "print(\"test set: \"+str(len(test_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eYL0FZ0IqNkE",
      "metadata": {
        "id": "eYL0FZ0IqNkE"
      },
      "source": [
        "Running the *shuffle_and_split_data* method again will give different sets. Note that seeing the whole dataset should be avoided which is why it should not be run too many times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Z8YzGquPxNFi",
      "metadata": {
        "id": "Z8YzGquPxNFi"
      },
      "outputs": [],
      "source": [
        "from zlib import crc32\n",
        "\n",
        "def is_id_in_test_set(identifier, test_ratio):\n",
        "  return crc32(np.int64(identifier)) < test_ratio *2**32\n",
        "\n",
        "def split_data_with_id_hash(data,test_ratio,id_column):\n",
        "  ids= data[id_column]\n",
        "  in_test_set = ids.apply(lambda id_:is_id_in_test_set(id_,test_ratio))\n",
        "  return data.loc[~in_test_set],data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y89cHJdsWZ8n",
      "metadata": {
        "id": "y89cHJdsWZ8n"
      },
      "source": [
        "The code above is a solution to the problem when running the program too many times. The solution is to use an identifier of an instance, and create a hash. The hash will will help determine if the instance belong in the test set or not by comparing the instance's hash to 20 percent of the mamximum hash value. If lower or equal to it then it may be put into the test set.\n",
        "\n",
        "The *is_id_in_test_set* method is a method that returns boolean values by comparing hashes as explained. *split_data_with_id_hash* will use the *is_id_in_test_set* method by:\n",
        " 1. Getting the the id column of the data to get the identifiers\n",
        " 2. Use the *is_id_in_test_set* method on each instances and putting the results in *in_test_set*.\n",
        " 3. Returns two lists of records, one not belonging in the testset and the other belonging.\n",
        "\n",
        "Since, the project doesn't seem to have an id column in it. An id column will have to be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "FffhVSnSa1Zm",
      "metadata": {
        "id": "FffhVSnSa1Zm"
      },
      "outputs": [],
      "source": [
        "housing_with_id = housing.reset_index()\n",
        "train_set,test_set = split_data_with_id_hash(housing_with_id,0.2,\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3pQ-TRgpbMS_",
      "metadata": {
        "id": "3pQ-TRgpbMS_"
      },
      "source": [
        "After that, split the data sets into subsets with scikit-learn's *train_test_split*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "h1fPMX0jcPjA",
      "metadata": {
        "id": "h1fPMX0jcPjA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5QcC5pQUdreh",
      "metadata": {
        "id": "5QcC5pQUdreh"
      },
      "source": [
        "Next, split it more into a bunch of strata to represent various groups of a population such as income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAEptxeYd2pa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "DAEptxeYd2pa",
        "outputId": "8ea1faad-1273-4a4f-cfce-56932ac20d3e"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],bins=[0,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])\n",
        "\n",
        "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0,grid=True)\n",
        "plt.xlabel(\"Income category\")\n",
        "plt.ylabel(\"Number of districts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "B8DBSITjf1Yg",
      "metadata": {
        "id": "B8DBSITjf1Yg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "splitter = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42)\n",
        "strat_splits = []\n",
        "for train_index,test_index in splitter.split(housing,housing[\"income_cat\"]):\n",
        "  strat_train_set_n = housing.iloc[train_index]\n",
        "  strat_test_set_n = housing.iloc[test_index]\n",
        "  strat_splits.append([strat_train_set_n,strat_test_set_n])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ySPjriLTg5-U",
      "metadata": {
        "id": "ySPjriLTg5-U"
      },
      "source": [
        "Split the test set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1X45Is-GhRJu",
      "metadata": {
        "id": "1X45Is-GhRJu"
      },
      "outputs": [],
      "source": [
        "strat_train_set,strat_test_set = strat_splits[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g1a4Um2DhmKB",
      "metadata": {
        "id": "g1a4Um2DhmKB"
      },
      "source": [
        "Taking a look at the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1e7mTThlie",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "bd1e7mTThlie",
        "outputId": "b8f0ed41-131c-4bcc-ee3d-afa4d804031c"
      },
      "outputs": [],
      "source": [
        "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iGpgR_-kiPzZ",
      "metadata": {
        "id": "iGpgR_-kiPzZ"
      },
      "source": [
        "Since the *income_cat* column won't be used again. It could be dropped by doing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eXewE8hRiPi6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXewE8hRiPi6",
        "outputId": "3ca2dbf9-32e1-4632-9e19-d8fbb732384f"
      },
      "outputs": [],
      "source": [
        "\n",
        "for set_ in (strat_train_set,strat_test_set):\n",
        "  set_.drop(\"income_cat\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Isdt0Q_uuj2U",
      "metadata": {
        "id": "Isdt0Q_uuj2U"
      },
      "source": [
        "### Exploring the data\n",
        "As the previous section has covered, the data has 10 attribute; 9 of them being floats and the last one being objects, including the newly added column to *housing*, *income_cat*. The relationship seems to be linear and the target attribute might just be income. The reason being that the result has already been tested and shown by previous research, so if the current project is done correctly according to the right procedures. There shouldn't be much of a difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyWhZpvCxmRM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyWhZpvCxmRM",
        "outputId": "c6f3b617-97d0-4ae5-b223-153d1ec98628"
      },
      "outputs": [],
      "source": [
        "housing.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5hC7UGRqR9KW",
      "metadata": {
        "id": "5hC7UGRqR9KW"
      },
      "source": [
        "The scatter plot for the current data is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZJPCneOiR87d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ZJPCneOiR87d",
        "outputId": "cdb67ef0-c7c5-4391-de5d-29a0155607af"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",grid=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GW5Nnb6XT2KV",
      "metadata": {
        "id": "GW5Nnb6XT2KV"
      },
      "source": [
        "Next, visualizing geograpical data, in the project, it will be visualized through a predefined color map called *jet*. In which, blue represents low values and red represents high values. The color of the dots represents price as the radius represents the population of a district."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ux98ytbdU8_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "ux98ytbdU8_z",
        "outputId": "f60d950f-5ecf-4d08-e316-cb7a970fc6b0"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",grid=True,s=housing[\"population\"]/100,label=\"population\",c=\"median_house_value\",cmap=\"jet\",colorbar=True,\n",
        "             legend=True,sharex=False,figsize=(10,7))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CdmlIUVkXCCy",
      "metadata": {
        "id": "CdmlIUVkXCCy"
      },
      "source": [
        "#### Correlation of data\n",
        "Looking for correlation is easy due to there already being a pre-defined method for printing correlation. The method is called corr and will show the correlation between the attributes and the median housing price or in other words, which attributes of the ten contributes more to the average pricing of houses in California."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tUcCIsz2XpDJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "tUcCIsz2XpDJ",
        "outputId": "766ab351-a222-4d98-85a2-e859212d0573"
      },
      "outputs": [],
      "source": [
        "housing = pd.get_dummies(housing, columns=['ocean_proximity'])\n",
        "\n",
        "\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uePjyBRlyUPj",
      "metadata": {
        "id": "uePjyBRlyUPj"
      },
      "source": [
        "The correlation method is done using the **standard correlation coefficient** method which has a correlation coefficient that ranges from -1 to 1. When it is -1, it means a strong negative correlation; when it is a 1, it means a strong positive correlation; when it is 0, it means a weak correlation or no linear correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHhk6trJ569H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "nHhk6trJ569H",
        "outputId": "2fdc9aa4-e20f-4b50-b735-a8f5173afafb"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DormCkDu6erV",
      "metadata": {
        "id": "DormCkDu6erV"
      },
      "source": [
        "Another way to check for correlation is to use the *scatter matrix* method which plots attributes with emphasis on their differences. Other variables or data to consider taking a look at are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e97sfO-Y7WXG",
      "metadata": {
        "id": "e97sfO-Y7WXG"
      },
      "outputs": [],
      "source": [
        "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
        "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
        "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DzlfHK_N7bww",
      "metadata": {
        "id": "DzlfHK_N7bww"
      },
      "source": [
        "Then use the *corr* method again to find the correlations of attributes to average housing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7j8C4vw7sTU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "V7j8C4vw7sTU",
        "outputId": "c463dc7f-9d7d-4d81-fb3e-b84eb59a5186"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRKsXWee9N8y",
      "metadata": {
        "id": "LRKsXWee9N8y"
      },
      "source": [
        "As could be seen, income is a bigger contributor to the average housing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "nNx8jPen9UF7",
      "metadata": {
        "id": "nNx8jPen9UF7"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ftiESTTj9vSl",
      "metadata": {
        "id": "ftiESTTj9vSl"
      },
      "source": [
        "#### Clean the data\n",
        "A new training set must be made in order to prepare the data for ML aglorithms. The first step is to clean the data set by 3 ways:\n",
        " 1. Getting rid of corresponding districts\n",
        " 2. Getting rid of whole attribute\n",
        " 3. imputation\n",
        "\n",
        "All three ways could be done easily through pre-defined methods. The main points of all three options are ways to clean data or more specifically, fix missing values. However, scikit provides an even easier way to fix missing values by having the *SimpleImputer* class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9025d7ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1 removing\n",
        "# the corresponding districts\n",
        "\n",
        "housing.drop(\"total_bedrooms\", axis=1)  # option 2 removing the whole attribute\n",
        "\n",
        "median = housing[\"total_bedrooms\"].median()  # option 3 Set the missing values\n",
        "housing[\"total_bedrooms\"].fillna(median, inplace=True) # to some value (zero,\n",
        "# the mean, the median, etc.). This is called imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "hAT9ihXR_EIh",
      "metadata": {
        "id": "hAT9ihXR_EIh"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "housing_num = housing.select_dtypes(include=[np.number])\n",
        "imputer.fit(housing_num)\n",
        "X = imputer.transform(housing_num)\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing_num.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y20FnmLLA8-y",
      "metadata": {
        "id": "y20FnmLLA8-y"
      },
      "source": [
        "#### Reflections\n",
        "If the project were to be done manually, it would take a much significant amount of time as the whole entire process up from the top to the bottom took a lot of steps. We would have to gather all the data, graph it, apply calculations such as correlations to get the correlation of the attributes to the median housing price. Then, in order to reuse the data, it must be copied and then cleaned as well. The process would take years to do if done manually.\n",
        "\n",
        "There are, however, some promising transformations that could be done to the data. One might be the z-score which provides good scaling in data. As for extra data that might be useful, we believe is the crime rates. The crime rates would be something I believe would be very crucial in determining the median house pricing as well.\n",
        "\n",
        "In conclusion, what we have learned so far is how to generate plots, how to use correlation, convenient tools that could be used for future data manipulation tools such as *drop*, and *dropna*. We also learned the process to look and analyze data as well. But, what could still be learned furthermore are the different tools, and their different usage in different contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bf9c8a",
      "metadata": {},
      "source": [
        "# Fine Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afec804",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Once we get the shortlist that it value is looking promising to use, we will need to set the best fine tune that will give us the best result. One option is to fiddle around with the hyperparameter manually. However, this proves it can be tedious work since there are multiple way. To reduce this, there are a couple of ways we can change the hyperparameter setting. The first way is Grid search. What it does is it will use cross-validation to evaluayed all the possible combination of hyperameter values. Here an example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa60f94",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
        "])\n",
        "param_grid = [\n",
        "    {'preprocessing__geo__n_clusters': [5, 8, 10],\n",
        "     'random_forest__max_features': [4, 6, 8]},\n",
        "    {'preprocessing__geo__n_clusters': [10, 15],\n",
        "     'random_forest__max_features': [6, 8, 10]},\n",
        "]\n",
        "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
        "                           scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa0504e",
      "metadata": {},
      "source": [
        "In this code, we have two dictionaries in the param_grid values. The grid Serach will evaluate the number of columns there are (in this case there are 3) so we do 3 x 3 = 9 combination of n_cluster and the other dictionaries will be 2 x 3 = 6 which it the combination of hyperparameter value. In total we get 15 (n_cluster + hyperparameter value) combination of hyperparameter value. The grid search does 3-fold cross validation. What this means is the new total is 45 (3x15) which means that it does 45 rounds of training. This will take a while but once it done then run the program\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f976148e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "cv_res.head()  # note: the 1st column is the row ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29de4c85",
      "metadata": {},
      "source": [
        "We will get the mean test score. We be focusing on is the mean_test_rmse. The lower the score the better the model will perform.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b9c42d",
      "metadata": {},
      "source": [
        "The other is randomizing search is often better than the grid search. When it comes to hyperparameter search space is large. It has it benefit like\n",
        "if your hyperparameters are continuous or discrete and need to do 1000 iteration, then it will explore 1000 different values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcfcce7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
        "                  'random_forest__max_features': randint(low=2, high=20)}\n",
        "\n",
        "rnd_search = RandomizedSearchCV(\n",
        "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
        "    scoring='neg_root_mean_squared_error', random_state=42)\n",
        "\n",
        "rnd_search.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b550b697",
      "metadata": {},
      "source": [
        "#### Ensemble Method\n",
        "\n",
        "\n",
        "\n",
        "Another way we can do is try to combine the models that perform the best. This is often better than the best Indvidual model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aff1f1a",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
