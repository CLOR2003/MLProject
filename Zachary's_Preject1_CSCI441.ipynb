{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07",
      "metadata": {
        "id": "8de7b3e0-2d34-49c6-a982-5e153535ab07"
      },
      "source": [
        "# Project 1\n",
        "## CSCI 441\n",
        "## Group 2\n",
        "## Members: Chakong Lor, Zachary Sunder, Luis Aguilar\n",
        "## Date: 10/4/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *TEMPORARY CELL\n",
        "### All prompts NEED to be answered before completion is acknowledged\n",
        "\n",
        "Questions to be answered for framing the problem:\n",
        "1. Define the objective in business terms.x\n",
        "2. How will your solution be used?x\n",
        "3. What are the current solutions/workarounds (if any)?x\n",
        "4. How should you frame this problem (supervised/unsupervised,\n",
        " online/offline, etc.)?x\n",
        "5. How should performance be measured?x\n",
        "6. Is the performance measure aligned with the business objective?x\n",
        "7. What would be the minimum performance needed to reach the\n",
        " business objective?x\n",
        "8. What are comparable problems? Can you reuse experience or\n",
        " tools?x\n",
        "9. Is human expertise available?x\n",
        "10. How would you solve the problem manually?x\n",
        "11. List the assumptions you (or others) have made so far.x\n",
        "12. Verify assumptions if possiblex\n",
        "\n",
        "Questions to be answered for Get the data:\n",
        " Note: automate as much as possible so you can easily get fresh data.\n",
        "1. List the data you need and how much you need.x\n",
        "2. Find and document where you can get that data.x\n",
        "3. Check how much space it will take.x\n",
        "4. Check legal obligations, and get authorization if necessary.x\n",
        "5. Get access authorizations.x\n",
        "6. Create a workspace (with enough storage space).x\n",
        "7. Get the data.x\n",
        "8. Convert the data to a format you can easily manipulate (without\n",
        " changing the data itself).x\n",
        "9. Ensure sensitive information is deleted or protected (e.g.,\n",
        " anonymized).x\n",
        "10. Check the size and type of data (time series, sample,\n",
        " geographical, etc.).x\n",
        "11. Sample a test set, put it aside, and never look at it (no data\n",
        " snooping!)x\n",
        "\n",
        "Questions to be answered for Explore the Data:\n",
        " Note: try to get insights from a field expert for these steps.\n",
        "1. Create a copy of the data for exploration (sampling it down to a\n",
        " manageable size if necessary).x\n",
        "2. Create a Jupyter notebook to keep a record of your data\n",
        " exploration.x\n",
        "3. Study each attribute and its characteristics:\n",
        " Name\n",
        " Type (categorical, int/float, bounded/unbounded, text,\n",
        " structured, etc.)\n",
        " % of missing values\n",
        " Noisiness and type of noise (stochastic, outliers,\n",
        " rounding errors, etc.)\n",
        " Usefulness for the task\n",
        " Type of distribution (Gaussian, uniform, logarithmic,\n",
        " etc.)x\n",
        "4. For supervised learning tasks, identify the target attribute(s).\n",
        "5. Visualize the data.\n",
        "6. Study the correlations between attributes.\n",
        "7. Study how you would solve the problem manually.\n",
        "8. Identify the promising transformations you may want to apply.\n",
        "9. Identify extra data that would be useful (go back to “Get the\n",
        " Data”).\n",
        "10. Document what you have learned.\n",
        "\n",
        "Questions to be answered for Prepare the Data: Zachary\n",
        " * Work on copies of the data (keep the original dataset intact).\n",
        " * Write functions for all data transformations you apply, for five\n",
        " reasons:\n",
        " * So you can easily prepare the data the next time you get a\n",
        " fresh dataset\n",
        "  * So you can apply these transformations in future projects\n",
        "  * To clean and prepare the test set\n",
        "  * To clean and prepare new data instances once your solution is live\n",
        "  * To make it easy to treat your preparation choices as\n",
        " hyperparameters\n",
        " 1. Data cleaning:\n",
        " Fix or remove outliers (optional).\n",
        " Fill in missing values (e.g., with zero, mean, median…)\n",
        " or drop their rows (or columns).\n",
        " 2. Feature selection (optional):\n",
        " Drop the attributes that provide no useful information for\n",
        " the task.\n",
        " 3. Feature engineering, where appropriate:\n",
        " Discretize continuous features.\n",
        " Decompose features (e.g., categorical, date/time, etc.).\n",
        " 2\n",
        " Add promising transformations of features (e.g., log(x),\n",
        " sqrt(x), x , etc.).\n",
        " Aggregate features into promising new features.\n",
        " 4. Feature scaling: Standardize or normalize features\n",
        "\n",
        "Shortlist Promising Models: Zachary\n",
        " * If the data is huge, you may want to sample smaller training sets\n",
        " so you can train many different models in a reasonable time (be\n",
        " aware that this penalizes complex models such as large neural\n",
        " nets or Random Forests).\n",
        " * Once again, try to automate these steps as much as possible.\n",
        " 1. Train many quick-and-dirty models from different categories\n",
        " (e.g., linear, naive Bayes, SVM, Random Forest, neural net, etc.)\n",
        " using standard parameters.\n",
        " 2. Measure and compare their performance.\n",
        " * For each model, use N-fold cross-validation and compute\n",
        " the mean and standard deviation of the performance\n",
        " measure on the N folds.\n",
        " 3. Analyze the most significant variables for each algorithm.\n",
        " 4. Analyze the types of errors the models make.\n",
        "What data would a human have used to avoid these\n",
        " errors?\n",
        " 5. Perform a quick round of feature selection and engineering.\n",
        " 6. Perform one or two more quick iterations of the five previous\n",
        " steps.\n",
        " 7. Shortlist the top three to five most promising models, preferring\n",
        " models that make different types of errors.\n",
        "\n",
        "Fine-Tune the System: Luis\n",
        "* You will want to use as much data as possible for this step,\n",
        " especially as you move toward the end of fine-tuning.\n",
        "* As always, automate what you can.\n",
        "1. Fine-tune the hyperparameters using cross-validation:\n",
        " * Treat your data transformation choices as\n",
        " hyperparameters, especially when you are not sure about\n",
        " them (e.g., if you’re not sure whether to replace missing\n",
        " values with zeros or with the median value, or to just\n",
        " drop the rows).\n",
        " * Unless there are very few hyperparameter values to\n",
        " explore, prefer random search over grid search. If\n",
        " training is very long, you may prefer a Bayesian\n",
        " optimization approach (e.g., using Gaussian process\n",
        " priors, as described by Jasper Snoek et al.).\n",
        " 1\n",
        "2. Try Ensemble methods. Combining your best models will often\n",
        " produce better performance than running them individually.\n",
        "3. Once you are confident about your final model, measure its\n",
        " performance on the test set to estimate the generalization error.\n",
        "\n",
        "Present Your Solution: Luis\n",
        " 1. Document what you have done.\n",
        " 2. Create a nice presentation.\n",
        " Make sure you highlight the big picture first.\n",
        " 3. Explain why your solution achieves the business objective.\n",
        " 4. Don’t forget to present interesting points you noticed along the\n",
        " way.\n",
        " Describe what worked and what did not.\n",
        " List your assumptions and your system’s limitations.\n",
        " 5. Ensure your key findings are communicated through beautiful\n",
        " visualizations or easy-to-remember statements (e.g., “the median\n",
        " income is the number-one predictor of housing prices”).\n",
        "\n",
        "Lastly: Luis\\\n",
        "Remember to write a summary at the bottom as to what we have learned."
      ],
      "metadata": {
        "id": "6nKs_XSD3T5N"
      },
      "id": "6nKs_XSD3T5N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare the Data"
      ],
      "metadata": {
        "id": "VKrMjwcuqMQh"
      },
      "id": "VKrMjwcuqMQh"
    },
    {
      "cell_type": "code",
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ],
      "metadata": {
        "id": "dz0WifFZsGnJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dz0WifFZsGnJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data Cleaning"
      ],
      "metadata": {
        "id": "sA0Yz1Xtvoiy"
      },
      "id": "sA0Yz1Xtvoiy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three options for cleaning the data where there are missing features, like the total_bedrooms attribute. The third option is the most reliable since it is the least destructive. The three options will also be listed below for reference:"
      ],
      "metadata": {
        "id": "H0gox-z79cmL"
      },
      "id": "H0gox-z79cmL"
    },
    {
      "cell_type": "code",
      "source": [
        "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1 removing\n",
        "# the corresponding districts\n",
        "\n",
        "housing.drop(\"total_bedrooms\", axis=1)  # option 2 removing the whole attribute\n",
        "\n",
        "median = housing[\"total_bedrooms\"].median()  # option 3 Set the missing values\n",
        "housing[\"total_bedrooms\"].fillna(median, inplace=True) # to some value (zero,\n",
        "# the mean, the median, etc.). This is called imputation"
      ],
      "metadata": {
        "id": "tduq4m0g-Gwu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tduq4m0g-Gwu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using the code as is from above though, we will be using the Scikit-Learn class called SimpleImputer because it is able to store the median value of each feature.\n",
        "\n",
        "To use it though, first it is needed to create a SimpleImputer instance and specify that you want to replace each attribute's missing values with the median of that attribute like so:"
      ],
      "metadata": {
        "id": "ecnl4VyB-yq_"
      },
      "id": "ecnl4VyB-yq_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "metadata": {
        "id": "IKNdM96j_h0k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IKNdM96j_h0k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the median can only be computed on numerical attributes though you will need to create a copy of the data with only the numerical attributes (this will exclude the text attribute ocean_proximity):"
      ],
      "metadata": {
        "id": "tlS4p_oq_twc"
      },
      "id": "tlS4p_oq_twc"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num = housing.select_dtypes(include=[np.number])"
      ],
      "metadata": {
        "id": "0k6hSPt5_2JG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0k6hSPt5_2JG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is possible to fit the imputer instance to the training data using the fit() method:"
      ],
      "metadata": {
        "id": "35ClkYNp_8wE"
      },
      "id": "35ClkYNp_8wE"
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(housing_num)"
      ],
      "metadata": {
        "id": "EbmLccBmADJZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EbmLccBmADJZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is much safer to apply the imputer to all the numerical attributes like so:"
      ],
      "metadata": {
        "id": "mz1Vy9tQANh7"
      },
      "id": "mz1Vy9tQANh7"
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.statistics_"
      ],
      "metadata": {
        "id": "y38GdstDAdKR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "y38GdstDAdKR"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num.median().values"
      ],
      "metadata": {
        "id": "wYBxsBKtAeVH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wYBxsBKtAeVH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is now possible to use this “trained” imputer to transform the training set by replacing missing values with the learned medians:"
      ],
      "metadata": {
        "id": "XXejSMkOAkx2"
      },
      "id": "XXejSMkOAkx2"
    },
    {
      "cell_type": "code",
      "source": [
        "X = imputer.transform(housing_num)"
      ],
      "metadata": {
        "id": "W_XMjRPoBDCF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W_XMjRPoBDCF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also other methods to replace the missing values such as replacing it with the mean using (strategy=\"mean\"), or with the most frequent value using (strategy=\"most_frequent\"), or with a constant value using (strategy=\"constant\", fill_value=...). Those last two methods also suppport non-numerical data.\n",
        "\n",
        "Now here are some further ways to transform the training set:"
      ],
      "metadata": {
        "id": "CO2QX3EKBOQS"
      },
      "id": "CO2QX3EKBOQS"
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.feature_names_in_"
      ],
      "metadata": {
        "id": "kni2UPorCwuf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kni2UPorCwuf"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing_num.index)"
      ],
      "metadata": {
        "id": "CpJu72sNC0pN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CpJu72sNC0pN"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_tr.loc[null_rows_idx].head()"
      ],
      "metadata": {
        "id": "hvcGnTlYCzos"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hvcGnTlYCzos"
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.strategy"
      ],
      "metadata": {
        "id": "yy5OXvgrC3Yu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yy5OXvgrC3Yu"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing_num.index)"
      ],
      "metadata": {
        "id": "eCw7qXN7C40A"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eCw7qXN7C40A"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_tr.loc[null_rows_idx].head()  # not shown in the book"
      ],
      "metadata": {
        "id": "Fo4TDBJHC6N-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Fo4TDBJHC6N-"
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn import set_config\n",
        "#\n",
        "# set_config(transform_output=\"pandas\")  # scikit-learn >= 1.2"
      ],
      "metadata": {
        "id": "JUQyHqHFC8Pk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JUQyHqHFC8Pk"
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_pred"
      ],
      "metadata": {
        "id": "ToXCEsuPDBw3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ToXCEsuPDBw3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To drop outliers run the following code:"
      ],
      "metadata": {
        "id": "raX_O8lvDEMQ"
      },
      "id": "raX_O8lvDEMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#housing = housing.iloc[outlier_pred == 1]\n",
        "#housing_labels = housing_labels.iloc[outlier_pred == 1]"
      ],
      "metadata": {
        "id": "0sbRsm3pDIBw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0sbRsm3pDIBw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Handling Text and Categorical Attributes"
      ],
      "metadata": {
        "id": "wTKeqCc5DKP2"
      },
      "id": "wTKeqCc5DKP2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point we have only dealt with numerical attributes, but there are also many cases where the data will contain text attributes.\n",
        "\n",
        "For example, the ocean_proximity attribute:"
      ],
      "metadata": {
        "id": "cN5PIqXEDaSd"
      },
      "id": "cN5PIqXEDaSd"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(8)"
      ],
      "metadata": {
        "id": "emlCITOVD4lv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "emlCITOVD4lv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most machine learning algorithms prefer to work with numbers, let's first convert the text to numbers using Scikit-Learn's OrdinalEncoder class like so:"
      ],
      "metadata": {
        "id": "44I6XxI2ETzr"
      },
      "id": "44I6XxI2ETzr"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
      ],
      "metadata": {
        "id": "ijFCfNrLEqsU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ijFCfNrLEqsU"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat_encoded[:8]"
      ],
      "metadata": {
        "id": "o0k7bJLXEuUf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "o0k7bJLXEuUf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):"
      ],
      "metadata": {
        "id": "rsTcXgx4E5M_"
      },
      "id": "rsTcXgx4E5M_"
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder.categories_"
      ],
      "metadata": {
        "id": "t_sC1q_MFBGK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "t_sC1q_MFBGK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is \"<1H OCEAN\" (and 0 otherwise), another attribute equal to 1 when the category is \"INLAND\" (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors:"
      ],
      "metadata": {
        "id": "ASmLXm_pFInt"
      },
      "id": "ASmLXm_pFInt"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
      ],
      "metadata": {
        "id": "7maD2jDiF6SO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7maD2jDiF6SO"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat_1hot"
      ],
      "metadata": {
        "id": "eqH5zOqYF-j8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eqH5zOqYF-j8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to convert the sparse matrix to a (dense) NumPy array, just call the toarray() method:"
      ],
      "metadata": {
        "id": "tWLXppJyF80Y"
      },
      "id": "tWLXppJyF80Y"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat_1hot.toarray()"
      ],
      "metadata": {
        "id": "pb3P2GA6G6Z1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pb3P2GA6G6Z1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To drop some outliers:"
      ],
      "metadata": {
        "id": "43j82lo6C9sa"
      },
      "id": "43j82lo6C9sa"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "outlier_pred = isolation_forest.fit_predict(X)"
      ],
      "metadata": {
        "id": "EUPAidXBDAjP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EUPAidXBDAjP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can set sparse_output=False when creating the OneHotEncoder (note: the sparse hyperparameter was renamned to sparse_output in Scikit-Learn 1.2):"
      ],
      "metadata": {
        "id": "uTJ2t5N6HCGA"
      },
      "id": "uTJ2t5N6HCGA"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ],
      "metadata": {
        "id": "pwhoQTNqHDtn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pwhoQTNqHDtn"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.categories_"
      ],
      "metadata": {
        "id": "Co74qk7zHH1K"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Co74qk7zHH1K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pandas function called get_dummies() converts each categorical feature into a one-hot representation, with one binary feature per category:"
      ],
      "metadata": {
        "id": "L-P4ob0XHNaP"
      },
      "id": "L-P4ob0XHNaP"
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
        "pd.get_dummies(df_test)"
      ],
      "metadata": {
        "id": "4tSNE4aaHM7s"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4tSNE4aaHM7s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of OneHotEncoder is that it remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. Look what our trained cat_encoder outputs when we make it transform the same df_test (using transform(), not fit_transform()):"
      ],
      "metadata": {
        "id": "D6E9hK5wHbWt"
      },
      "id": "D6E9hK5wHbWt"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.transform(df_test)"
      ],
      "metadata": {
        "id": "JR7I71m4Hc_n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JR7I71m4Hc_n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since get_dummies() saw only two categories, it output two columns, whereas OneHotEncoder output one column per learned category, in the right order. Moreover, if you feed get_dummies() a DataFrame containing an unknown category (e.g., \"<2H OCEAN\"), it will happily generate a column for it:"
      ],
      "metadata": {
        "id": "jns-yfTWHjCD"
      },
      "id": "jns-yfTWHjCD"
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
        "pd.get_dummies(df_test_unknown)"
      ],
      "metadata": {
        "id": "MoMvf2SVHfHt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MoMvf2SVHfHt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "OneHotEncoder is smarter: it will detect the unknown category and raise an exception. If you prefer, you can set the handle_unknown hyperparameter to \"ignore\", in which case it will just represent the unknown category with zeros:"
      ],
      "metadata": {
        "id": "95ZvTmnzHyng"
      },
      "id": "95ZvTmnzHyng"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.handle_unknown = \"ignore\"\n",
        "cat_encoder.transform(df_test_unknown)"
      ],
      "metadata": {
        "id": "T_eVtcxEH39V"
      },
      "execution_count": null,
      "outputs": [],
      "id": "T_eVtcxEH39V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you fit any Scikit-Learn estimator using a DataFrame, the estimator stores the column names in the feature_names_in_ attribute. Scikit-Learn then ensures that any DataFrame fed to this estimator after that (e.g., to transform() or predict()) has the same column names. Transformers also provide a get_feature_names_out() method that you can use to build a DataFrame around the transformer’s output:"
      ],
      "metadata": {
        "id": "oitPi6YLH-9e"
      },
      "id": "oitPi6YLH-9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb9jFOu9nva7",
        "outputId": "2d8e382e-349a-4696-9488-7a87635e15f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ocean_proximity'], dtype=object)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cat_encoder.feature_names_in_"
      ],
      "id": "hb9jFOu9nva7"
    },
    {
      "cell_type": "code",
      "source": [
        "cat_encoder.get_feature_names_out()"
      ],
      "metadata": {
        "id": "cXmljgL8IIoE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cXmljgL8IIoE"
    },
    {
      "cell_type": "code",
      "source": [
        "df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n",
        "                         columns=cat_encoder.get_feature_names_out(),\n",
        "                         index=df_test_unknown.index)"
      ],
      "metadata": {
        "id": "xBXdPShxIK0n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xBXdPShxIK0n"
    },
    {
      "cell_type": "code",
      "source": [
        "df_output"
      ],
      "metadata": {
        "id": "2fnJ7iuvINq8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2fnJ7iuvINq8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Feature Scaling"
      ],
      "metadata": {
        "id": "A_w-jXinIOTz"
      },
      "id": "A_w-jXinIOTz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is one of the most important transformations that will need to be applied to your data. Machine learning algorithms don’t generally perform well when the input of numerical attributes have very different scales. This is the case for  housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n",
        "\n",
        "The two main ways to get the attributes to have the same scale are: min-max  scaling (also known as normalization) and standardization.\n",
        "\n",
        "Scikit-Learn provides a transformer called MinMaxScaler for min-max scaling. In our case, we will want to set the range from (-1, 1) since that is what neural networks work best with:"
      ],
      "metadata": {
        "id": "RjUswH9MIQrB"
      },
      "id": "RjUswH9MIQrB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
      ],
      "metadata": {
        "id": "ktBGU3cWJmDg"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ktBGU3cWJmDg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a similar way, Scikit-Learn provides a transformer called StandardScaler for standardization:"
      ],
      "metadata": {
        "id": "H-Vah1fCJpQP"
      },
      "id": "H-Vah1fCJpQP"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
      ],
      "metadata": {
        "id": "VQiaElnnKcg6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VQiaElnnKcg6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. Machine learning models generally don’t like this at all. Before you scaling the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical.\n",
        "\n",
        "For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help.\n",
        "\n",
        "For example, the population feature roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Figure 2-17 shows how much better this feature looks when you compute its log: it’s very close to a Gaussian distribution (i.e., bell-shaped)."
      ],
      "metadata": {
        "id": "c4GZ3xFLKeWW"
      },
      "id": "c4GZ3xFLKeWW"
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates Figure 2–17\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
        "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
        "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
        "axs[0].set_xlabel(\"Population\")\n",
        "axs[1].set_xlabel(\"Log of population\")\n",
        "axs[0].set_ylabel(\"Number of districts\")\n",
        "save_fig(\"long_tail_plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7QIqpFPULS0c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7QIqpFPULS0c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach to handling heavy-tailed features is bucketizing the feature. This is especially the case if the feature has a multimodal distribution, such as the housing_median_age feature. Another approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF) like so:"
      ],
      "metadata": {
        "id": "ibRG97ZyNGe1"
      },
      "id": "ibRG97ZyNGe1"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)"
      ],
      "metadata": {
        "id": "wvL0zqevPdM6"
      },
      "id": "wvL0zqevPdM6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates Figure 2–18\n",
        "\n",
        "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
        "                   housing[\"housing_median_age\"].max(),\n",
        "                   500).reshape(-1, 1)\n",
        "gamma1 = 0.1\n",
        "gamma2 = 0.03\n",
        "rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\n",
        "rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel(\"Housing median age\")\n",
        "ax1.set_ylabel(\"Number of districts\")\n",
        "ax1.hist(housing[\"housing_median_age\"], bins=50)\n",
        "\n",
        "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
        "color = \"blue\"\n",
        "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
        "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "ax2.set_ylabel(\"Age similarity\", color=color)\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "save_fig(\"age_similarity_plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AYasLix_Pezo"
      },
      "id": "AYasLix_Pezo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also helpful tools to help transform the target values. One way is to use the LinearRegression class from Scikit-Learn like so:"
      ],
      "metadata": {
        "id": "KlclX3H2Pwqk"
      },
      "id": "KlclX3H2Pwqk"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "target_scaler = StandardScaler()\n",
        "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
        "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
        "\n",
        "scaled_predictions = model.predict(some_new_data)\n",
        "predictions = target_scaler.inverse_transform(scaled_predictions)"
      ],
      "metadata": {
        "id": "rh73KwlCQ6Rq"
      },
      "id": "rh73KwlCQ6Rq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "Q3taGfMoQ9ux"
      },
      "id": "Q3taGfMoQ9ux",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a simpler way of doing this is which is to use Scikit-Learn's TransformedTargetRegressor class as shown below:"
      ],
      "metadata": {
        "id": "fibgQz7OQ-Ps"
      },
      "id": "fibgQz7OQ-Ps"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "model = TransformedTargetRegressor(LinearRegression(),\n",
        "                                   transformer=StandardScaler())\n",
        "model.fit(housing[[\"median_income\"]], housing_labels)\n",
        "predictions = model.predict(some_new_data)"
      ],
      "metadata": {
        "id": "EQvvKep6RTqv"
      },
      "id": "EQvvKep6RTqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "T5O_rwHyRXGY"
      },
      "id": "T5O_rwHyRXGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Custom Transformers"
      ],
      "metadata": {
        "id": "yx1obiSxRYG1"
      },
      "id": "yx1obiSxRYG1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom transformations, cleanup operations, or combining specific attributes.\n",
        "\n",
        "For example, creating a log-transformer and applying it to the population feature:"
      ],
      "metadata": {
        "id": "kZSlSl8KRcF3"
      },
      "id": "kZSlSl8KRcF3"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
        "log_pop = log_transformer.transform(housing[[\"population\"]])"
      ],
      "metadata": {
        "id": "ZdX4W8gpSHtW"
      },
      "id": "ZdX4W8gpSHtW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a transformer that computes the same Gaussian RBF similarity measure as earlier:"
      ],
      "metadata": {
        "id": "UrcnvjpESPV_"
      },
      "id": "UrcnvjpESPV_"
    },
    {
      "cell_type": "code",
      "source": [
        "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
        "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
      ],
      "metadata": {
        "id": "1mywkX8mSGy2"
      },
      "id": "1mywkX8mSGy2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_simil_35"
      ],
      "metadata": {
        "id": "7sO03jR8SRGL"
      },
      "id": "7sO03jR8SRGL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a feature that will measure the geographic similarity between each district and San Francisco:"
      ],
      "metadata": {
        "id": "BYQyyq1JTAgi"
      },
      "id": "BYQyyq1JTAgi"
    },
    {
      "cell_type": "code",
      "source": [
        "sf_coords = 37.7749, -122.41\n",
        "sf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\n",
        "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
      ],
      "metadata": {
        "id": "LxAhle9HTKIl"
      },
      "id": "LxAhle9HTKIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf_simil"
      ],
      "metadata": {
        "id": "LCmDVeEkTL_D"
      },
      "id": "LCmDVeEkTL_D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom transformers are also able to combine features, like FunctionTransformer that computes the ratio between the input features 0 and 1:"
      ],
      "metadata": {
        "id": "Ia_RwKMKTUfb"
      },
      "id": "Ia_RwKMKTUfb"
    },
    {
      "cell_type": "code",
      "source": [
        "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
        "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))"
      ],
      "metadata": {
        "id": "fqJ2n4uIThbm"
      },
      "id": "fqJ2n4uIThbm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make FunctionTransformer trainable you would need to create a custom class that has these three methods: fit() (which must return self), transform(), and fit_transform().\n",
        "\n",
        "As an example, here's a custom transformer class that acts similarly to StandardScaler:"
      ],
      "metadata": {
        "id": "O7fR2H8-Tu-P"
      },
      "id": "O7fR2H8-Tu-P"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_array, check_is_fitted\n",
        "\n",
        "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
        "        self.with_mean = with_mean\n",
        "\n",
        "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
        "        X = check_array(X)  # checks that X is an array with finite float values\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        self.scale_ = X.std(axis=0)\n",
        "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
        "        X = check_array(X)\n",
        "        assert self.n_features_in_ == X.shape[1]\n",
        "        if self.with_mean:\n",
        "            X = X - self.mean_\n",
        "        return X / self.scale_"
      ],
      "metadata": {
        "id": "mV7kdiAsUdwa"
      },
      "id": "mV7kdiAsUdwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom transformer can (and often does) use other estimators in its implementation. Here is an example of one:"
      ],
      "metadata": {
        "id": "mxGUGSBpUi7h"
      },
      "id": "mxGUGSBpUi7h"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.gamma = gamma\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
        "                              random_state=self.random_state)\n",
        "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
        "\n",
        "    def get_feature_names_out(self, names=None):\n",
        "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
      ],
      "metadata": {
        "id": "WdUP9FV_U3sH"
      },
      "id": "WdUP9FV_U3sH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting this custom transformer into use:"
      ],
      "metadata": {
        "id": "YnsvTkshVFt4"
      },
      "id": "YnsvTkshVFt4"
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
        "                                           sample_weight=housing_labels)"
      ],
      "metadata": {
        "id": "RhmRU1TzVKQF"
      },
      "id": "RhmRU1TzVKQF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a ClusterSimilarity transformer with 10 clusters. To look at the first three rows while rounding to two decimal places we call the following:"
      ],
      "metadata": {
        "id": "UFl4o5NSVWDk"
      },
      "id": "UFl4o5NSVWDk"
    },
    {
      "cell_type": "code",
      "source": [
        "similarities[:3].round(2)"
      ],
      "metadata": {
        "id": "Zjaljd3ZVLoO"
      },
      "id": "Zjaljd3ZVLoO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following shows the 10 clusters colored according to their geographic similarity to their closest cluster center:"
      ],
      "metadata": {
        "id": "2cmrvnH3V_5o"
      },
      "id": "2cmrvnH3V_5o"
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates Figure 2–19\n",
        "\n",
        "housing_renamed = housing.rename(columns={\n",
        "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
        "    \"population\": \"Population\",\n",
        "    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
        "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
        "\n",
        "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
        "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
        "                     c=\"Max cluster similarity\",\n",
        "                     cmap=\"jet\", colorbar=True,\n",
        "                     legend=True, sharex=False, figsize=(10, 7))\n",
        "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
        "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
        "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
        "         label=\"Cluster centers\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "save_fig(\"district_cluster_plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KUiZF-fYWNZg"
      },
      "id": "KUiZF-fYWNZg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Transformation Pipelines"
      ],
      "metadata": {
        "id": "HzNSVQiNWPlN"
      },
      "id": "HzNSVQiNWPlN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better facilitate the data transformation steps Scikit-Learn has the Pipeline class. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:"
      ],
      "metadata": {
        "id": "HoFaTmZZWS1k"
      },
      "id": "HoFaTmZZWS1k"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"standardize\", StandardScaler()),\n",
        "])"
      ],
      "metadata": {
        "id": "kGXElheTW-jp"
      },
      "id": "kGXElheTW-jp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also the option to use the make_pipeline() function instead; it takes transformers as positional arguments and creates a Pipeline using the names of the transformers’ classes, in lowercase and without underscores (e.g., \"simpleimputer\"):"
      ],
      "metadata": {
        "id": "VXkfjkoiXSNY"
      },
      "id": "VXkfjkoiXSNY"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
      ],
      "metadata": {
        "id": "an5rCAc1Xd2z"
      },
      "id": "an5rCAc1Xd2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "set_config(display='diagram')\n",
        "\n",
        "num_pipeline"
      ],
      "metadata": {
        "id": "kxhEyamCX5sw"
      },
      "id": "kxhEyamCX5sw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the pipeline’s fit_transform() method and looking at the output’s first two rows, rounded to two decimal places:"
      ],
      "metadata": {
        "id": "-oMTEM_sXs_Q"
      },
      "id": "-oMTEM_sXs_Q"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
        "housing_num_prepared[:2].round(2)"
      ],
      "metadata": {
        "id": "VTjSe9COX1lc"
      },
      "id": "VTjSe9COX1lc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the pipeline’s get_feature_names_out() method to recover a nice DataFrame:"
      ],
      "metadata": {
        "id": "0Ehpf5AOf4My"
      },
      "id": "0Ehpf5AOf4My"
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_num_prepared = pd.DataFrame(\n",
        "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
        "    index=housing_num.index)"
      ],
      "metadata": {
        "id": "PJRS3Y7xgBsg"
      },
      "id": "PJRS3Y7xgBsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_num_prepared.head(2)  # extra code"
      ],
      "metadata": {
        "id": "I5qwyvi6gFKo"
      },
      "id": "I5qwyvi6gFKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to have a single transformer that is capable of handling all columns and applying the appropriate transformations on each column using the ColumnTransformer class from Scikit-Learn:"
      ],
      "metadata": {
        "id": "MF3No_24gQrB"
      },
      "id": "MF3No_24gQrB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
        "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "cat_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", cat_pipeline, cat_attribs),\n",
        "])"
      ],
      "metadata": {
        "id": "fx1xa2NqgrBz"
      },
      "id": "fx1xa2NqgrBz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing make_column_selector to make_column_transformer to automatically name the transformer and select all features of a given type:"
      ],
      "metadata": {
        "id": "Yj2Chr9jhCgr"
      },
      "id": "Yj2Chr9jhCgr"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "\n",
        "preprocessing = make_column_transformer(\n",
        "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
        "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
        ")"
      ],
      "metadata": {
        "id": "VZT2cUyFhbOc"
      },
      "id": "VZT2cUyFhbOc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying ColumnTransformer to the housing data:"
      ],
      "metadata": {
        "id": "KzRj-3qEhfZ5"
      },
      "id": "KzRj-3qEhfZ5"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)"
      ],
      "metadata": {
        "id": "cIKaCLI2hlEB"
      },
      "id": "cIKaCLI2hlEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a single pipeline that will perform all the transformations that have been experimented with up to now. The following code builds the pipeline to do all of this:"
      ],
      "metadata": {
        "id": "03lTvNkKhskF"
      },
      "id": "03lTvNkKhskF"
    },
    {
      "cell_type": "code",
      "source": [
        "def column_ratio(X):\n",
        "    return X[:, [0]] / X[:, [1]]\n",
        "\n",
        "def ratio_name(function_transformer, feature_names_in):\n",
        "    return [\"ratio\"]  # feature names out\n",
        "\n",
        "def ratio_pipeline():\n",
        "    return make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
        "        StandardScaler())\n",
        "\n",
        "log_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
        "    StandardScaler())\n",
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
        "                                     StandardScaler())\n",
        "preprocessing = ColumnTransformer([\n",
        "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
        "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
        "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
        "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
        "                               \"households\", \"median_income\"]),\n",
        "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
        "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
        "    ],\n",
        "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
      ],
      "metadata": {
        "id": "3joK9Xh6iCkD"
      },
      "id": "3joK9Xh6iCkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the functionality of the code:"
      ],
      "metadata": {
        "id": "K24qn6JIiHAx"
      },
      "id": "K24qn6JIiHAx"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)\n",
        "housing_prepared.shape"
      ],
      "metadata": {
        "id": "feG8vGAxiKp_"
      },
      "id": "feG8vGAxiKp_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing.get_feature_names_out()"
      ],
      "metadata": {
        "id": "dqeUvKjtiOVQ"
      },
      "id": "dqeUvKjtiOVQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Select and Train a Model"
      ],
      "metadata": {
        "id": "AFieYYH3iayd"
      },
      "id": "AFieYYH3iayd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training and Evaluating on the Training Set"
      ],
      "metadata": {
        "id": "bu54BZUFid0p"
      },
      "id": "bu54BZUFid0p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting off with training a basic linear regression model:"
      ],
      "metadata": {
        "id": "I5PXUq1Qio9q"
      },
      "id": "I5PXUq1Qio9q"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
        "lin_reg.fit(housing, housing_labels)"
      ],
      "metadata": {
        "id": "1O4T9pdaixgb"
      },
      "id": "1O4T9pdaixgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the linear regression model on the training set:"
      ],
      "metadata": {
        "id": "CEV3EZx3jAWj"
      },
      "id": "CEV3EZx3jAWj"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_predictions = lin_reg.predict(housing)\n",
        "housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred"
      ],
      "metadata": {
        "id": "CL4EFh_njIOn"
      },
      "id": "CL4EFh_njIOn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And comparing against the actual values:"
      ],
      "metadata": {
        "id": "RQXFThV9jK96"
      },
      "id": "RQXFThV9jK96"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_labels.iloc[:5].values"
      ],
      "metadata": {
        "id": "Hw3Ngz5NjOnb"
      },
      "id": "Hw3Ngz5NjOnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function, with the squared argument set to False:"
      ],
      "metadata": {
        "id": "YhxBx0WFjfPe"
      },
      "id": "YhxBx0WFjfPe"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                              squared=False)\n",
        "lin_rmse"
      ],
      "metadata": {
        "id": "jkIN-ovMji2v"
      },
      "id": "jkIN-ovMji2v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model isn't working as well as what was hoped for we need to go about changing something. The three general options are: selecting a more powerful model, feeding the training algorithm with better features, or reducing the constraints on the model.\n",
        "\n",
        "The last option isn't realistic since the model isn't regularized, so we will go with the option of selecting a more powerful model. That model will be the DecisionTreeRegressor model as shown in the code below:"
      ],
      "metadata": {
        "id": "8wsGb7MMjoum"
      },
      "id": "8wsGb7MMjoum"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
        "tree_reg.fit(housing, housing_labels)"
      ],
      "metadata": {
        "id": "eQiF3nO4knJS"
      },
      "id": "eQiF3nO4knJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the newly trained DecisionTreeRegressor model on the training set:"
      ],
      "metadata": {
        "id": "MBAmLN6OkqCT"
      },
      "id": "MBAmLN6OkqCT"
    },
    {
      "cell_type": "code",
      "source": [
        "housing_predictions = tree_reg.predict(housing)\n",
        "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                              squared=False)\n",
        "tree_rmse"
      ],
      "metadata": {
        "id": "NXD_qw_JkxZG"
      },
      "id": "NXD_qw_JkxZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before finalizing anything or changing the test set, we will move onto model validation."
      ],
      "metadata": {
        "id": "57JieOzflG_6"
      },
      "id": "57JieOzflG_6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Better Evaluation Using Cross-Validation"
      ],
      "metadata": {
        "id": "mTEIpRZtk5AU"
      },
      "id": "mTEIpRZtk5AU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step of this process is about verifying whether the DecisionTreeRegressor model is correctly fit for the expected data.\n",
        "\n",
        "One great way to test that is to use Scikit-Learn’s k_-fold cross-validation feature. The following code will test and evaluate the model 10 different times and give an array containing the 10 evaluation scores:"
      ],
      "metadata": {
        "id": "pTKirrqhk9at"
      },
      "id": "pTKirrqhk9at"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
      ],
      "metadata": {
        "id": "Ec9vYMlkmeaa"
      },
      "id": "Ec9vYMlkmeaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the results:"
      ],
      "metadata": {
        "id": "bgGvJ6iqmjaK"
      },
      "id": "bgGvJ6iqmjaK"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(tree_rmses).describe()"
      ],
      "metadata": {
        "id": "nsCGRs-rmfTc"
      },
      "id": "nsCGRs-rmfTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showcasing the error stats for the LinearRegression model:"
      ],
      "metadata": {
        "id": "gLgAMt3Nn34I"
      },
      "id": "gLgAMt3Nn34I"
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – computes the error stats for the linear model\n",
        "lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
        "pd.Series(lin_rmses).describe()"
      ],
      "metadata": {
        "id": "oCuysBNon3NX"
      },
      "id": "oCuysBNon3NX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After doing this cross-validation, it can be seen that the DecisionTreeRegressor model performs similarly to the LinearRegression model. Although the process of cross-validation can provide you will valuable information about the model, sometimes it isn't always realistic because you will then need to train the model several times.\n",
        "\n",
        "The last model that we will be trying is the RandomForestRegressor model and the code is shown below:"
      ],
      "metadata": {
        "id": "PRp4FX13mrJq"
      },
      "id": "PRp4FX13mrJq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = make_pipeline(preprocessing,\n",
        "                           RandomForestRegressor(random_state=42))\n",
        "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
        "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
      ],
      "metadata": {
        "id": "qOE-zxrznwdE"
      },
      "id": "qOE-zxrznwdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(forest_rmses).describe()"
      ],
      "metadata": {
        "id": "6YeKQAKUoETv"
      },
      "id": "6YeKQAKUoETv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showcasing the RMSE for the RandomForestRegressor model on the training set:"
      ],
      "metadata": {
        "id": "7QVK5dSsomRf"
      },
      "id": "7QVK5dSsomRf"
    },
    {
      "cell_type": "code",
      "source": [
        "forest_reg.fit(housing, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing)\n",
        "forest_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                                 squared=False)\n",
        "forest_rmse"
      ],
      "metadata": {
        "id": "RJFoqeQyol63"
      },
      "id": "RJFoqeQyol63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing the new RandomForestRegressor model, it can be seen that it is much more accurate than what the previous two were, but it can also be seen that there is still a lot of overfitting as shown by the RMSE.\n",
        "\n",
        "Some potential solutions are to simplify the model, constrain the model, or to get more training data. This is where it comes down to shortlisting a few promising models after testing models from various categories of machine learning algorithms without worrying too much about the hyperparameters."
      ],
      "metadata": {
        "id": "z_SkRAl8oBq8"
      },
      "id": "z_SkRAl8oBq8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}